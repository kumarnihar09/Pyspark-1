{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8fef7d6-6b4e-4ca2-884b-364a0eb3a06b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Write a pyspark dataframe query to find all duplicate emails in a table named Person.\n",
    "\n",
    "+----+---------+ | Id | Email | +----+---------+ | 1 | a@b.com | | 2 | c@d.com | | 3 | a@b.com | +----+---------+ For example, your query should return the following for the above table:\n",
    "\n",
    "+---------+ | Email | +---------+ | a@b.com | +---------+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bef09362-3c20-4049-b52d-98a751c4f693",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a70d66a-8e8d-4a92-afbb-4b9eae256566",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n| Id|  Email|\n+---+-------+\n|  1|a@b.com|\n|  2|c@d.com|\n|  3|a@b.com|\n+---+-------+\n\n+-------+\n|  Email|\n+-------+\n|a@b.com|\n+-------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import count\n",
    "spark = SparkSession.builder.appName(\"Person\").getOrCreate()\n",
    "data = [\n",
    "    (1, \"a@b.com\"),\n",
    "    (2, \"c@d.com\"),\n",
    "    (3, \"a@b.com\")\n",
    "]\n",
    "columns = [\"Id\", \"Email\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "\n",
    "\n",
    "# Find duplicate emails\n",
    "df_result = df.groupBy('Email').count().filter('count > 1').select('Email')\n",
    "\n",
    "# Show the result\n",
    "df_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b69f8189-77d5-4821-b8b8-204310db05e7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+-------------+\n|customer_id|purchase_amount|purchase_date|\n+-----------+---------------+-------------+\n|          1|            100|   2023-01-15|\n|          2|            150|   2023-02-20|\n|          1|            200|   2023-03-10|\n|          3|             50|   2023-04-05|\n|          2|            120|   2023-05-15|\n|          1|            300|   2023-06-25|\n+-----------+---------------+-------------+\n\n+-----------+---------------------+\n|customer_id|total_purchase_amount|\n+-----------+---------------------+\n|          1|                  600|\n|          2|                  270|\n|          3|                   50|\n+-----------+---------------------+\n\n"
     ]
    }
   ],
   "source": [
    "##Calculate the total purchase amount for each customer:\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum\n",
    "spark = SparkSession.builder.appName(\"CustomerPurchaseAnalysis\").getOrCreate()\n",
    "data = [\n",
    "    (1, 100, \"2023-01-15\"),\n",
    "    (2, 150, \"2023-02-20\"),\n",
    "    (1, 200, \"2023-03-10\"),\n",
    "    (3, 50, \"2023-04-05\"),\n",
    "    (2, 120, \"2023-05-15\"),\n",
    "    (1, 300, \"2023-06-25\")\n",
    "]\n",
    "columns = [\"customer_id\", \"purchase_amount\", \"purchase_date\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "total_purchase_per_customer = df.groupBy('customer_id').agg(sum('purchase_amount').alias('total_purchase_amount'))\n",
    "total_purchase_per_customer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b2de1f5-a0d4-4d12-84ff-635147342136",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+-------------+\n|customer_id|purchase_amount|purchase_date|\n+-----------+---------------+-------------+\n|          1|            100|   2023-01-15|\n|          2|            150|   2023-02-20|\n|          1|            200|   2023-03-10|\n|          3|             50|   2023-04-05|\n|          2|            120|   2023-05-15|\n|          1|            300|   2023-06-25|\n+-----------+---------------+-------------+\n\nOut[4]: 1"
     ]
    }
   ],
   "source": [
    "#Find the customer with the highest total purchase amount\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum, col, desc\n",
    "spark = SparkSession.builder.appName(\"CustomerPurchaseAnalysis\").getOrCreate()\n",
    "data = [\n",
    "    (1, 100, \"2023-01-15\"),\n",
    "    (2, 150, \"2023-02-20\"),\n",
    "    (1, 200, \"2023-03-10\"),\n",
    "    (3, 50, \"2023-04-05\"),\n",
    "    (2, 120, \"2023-05-15\"),\n",
    "    (1, 300, \"2023-06-25\")\n",
    "]\n",
    "columns = [\"customer_id\", \"purchase_amount\", \"purchase_date\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "total_purchase_per_customer = df.groupBy('customer_id').agg(sum('purchase_amount').alias('total_purchase_amount'))\n",
    "customer_with_highest_purchase = total_purchase_per_customer.orderBy(desc('total_purchase_amount')).first()\n",
    "#the above produces row not dataset so cant use show\n",
    "customer_with_highest_purchase['customer_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fa83b13-a82a-4070-8d7a-a9047e0d9573",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----------+-----+-------------+\n|product_id|product_name|   category|price|quantity_sold|\n+----------+------------+-----------+-----+-------------+\n|         1|   Product A|Electronics|  500|          100|\n|         2|   Product B|   Clothing|   50|          200|\n|         3|   Product C|Electronics|  800|           50|\n|         4|   Product D|     Beauty|   30|          300|\n|         5|   Product E|   Clothing|   75|          150|\n+----------+------------+-----------+-----+-------------+\n\n+-------------------+\n|total_rev_all_sales|\n+-------------------+\n|             120250|\n+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Q1.Calculate the total revenue generated from all sales.     \n",
    "          \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum, col, avg\n",
    "from pyspark.sql.window import Window\n",
    "spark = SparkSession.builder.appName(\"Sales\").getOrCreate()\n",
    "data = [\n",
    "    (1, \"Product A\", \"Electronics\",500,100),\n",
    "    (2, \"Product B\", \"Clothing\",50,200),\n",
    "    (3, \"Product C\", \"Electronics\",800,50),\n",
    "    (4, \"Product D\", \"Beauty\",30,300),\n",
    "    (5, \"Product E\", \"Clothing\",75,150)\n",
    "]\n",
    "columns = [\"product_id\", \"product_name\", \"category\",\"price\",\"quantity_sold\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "df.withColumn('total_revenue',col('price')*col('quantity_sold')).agg(sum('total_revenue').alias('total_rev_all_sales')).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06476e84-975e-4cfe-82f4-fff83c310d4e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----------+-----+-------------+\n|product_id|product_name|   category|price|quantity_sold|\n+----------+------------+-----------+-----+-------------+\n|         4|   Product D|     Beauty|   30|          300|\n|         2|   Product B|   Clothing|   50|          200|\n|         5|   Product E|   Clothing|   75|          150|\n|         1|   Product A|Electronics|  500|          100|\n|         3|   Product C|Electronics|  800|           50|\n+----------+------------+-----------+-----+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "## Q2.Find the top 5 best-selling products based on the quantity sold.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum, col, avg\n",
    "from pyspark.sql.window import Window\n",
    "spark = SparkSession.builder.appName(\"Sales\").getOrCreate()\n",
    "data = [\n",
    "    (1, \"Product A\", \"Electronics\",500,100),\n",
    "    (2, \"Product B\", \"Clothing\",50,200),\n",
    "    (3, \"Product C\", \"Electronics\",800,50),\n",
    "    (4, \"Product D\", \"Beauty\",30,300),\n",
    "    (5, \"Product E\", \"Clothing\",75,150)\n",
    "]\n",
    "columns = [\"product_id\", \"product_name\", \"category\",\"price\",\"quantity_sold\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "# df.show()\n",
    "df.orderBy(col('quantity_sold').desc()).limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a201c236-4abd-46b4-9042-3431f68670bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----------+-----+-------------+\n|product_id|product_name|   category|price|quantity_sold|\n+----------+------------+-----------+-----+-------------+\n|         1|   Product A|Electronics|  500|          100|\n|         2|   Product B|   Clothing|   50|          200|\n|         3|   Product C|Electronics|  800|           50|\n|         4|   Product D|     Beauty|   30|          300|\n|         5|   Product E|   Clothing|   75|          150|\n+----------+------------+-----------+-----+-------------+\n\n+-----------+----------------+\n|   category|Avg_per_category|\n+-----------+----------------+\n|Electronics|           650.0|\n|   Clothing|            62.5|\n|     Beauty|            30.0|\n+-----------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "#Q3.Calculate the average price of products in each category.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum, col, avg\n",
    "from pyspark.sql.window import Window\n",
    "spark = SparkSession.builder.appName(\"Sales\").getOrCreate()\n",
    "data = [\n",
    "    (1, \"Product A\", \"Electronics\",500,100),\n",
    "    (2, \"Product B\", \"Clothing\",50,200),\n",
    "    (3, \"Product C\", \"Electronics\",800,50),\n",
    "    (4, \"Product D\", \"Beauty\",30,300),\n",
    "    (5, \"Product E\", \"Clothing\",75,150)\n",
    "]\n",
    "columns = [\"product_id\", \"product_name\", \"category\",\"price\",\"quantity_sold\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "df.groupBy('category').agg(avg('price').alias('Avg_per_category')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64c95ca0-ce3a-49d9-b9b7-eca49b8eaa9f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----------+-----+-------------+\n|product_id|product_name|   category|price|quantity_sold|\n+----------+------------+-----------+-----+-------------+\n|         1|   Product A|Electronics|  500|          100|\n|         2|   Product B|   Clothing|   50|          200|\n|         3|   Product C|Electronics|  800|           50|\n|         4|   Product D|     Beauty|   30|          300|\n|         5|   Product E|   Clothing|   75|          150|\n+----------+------------+-----------+-----+-------------+\n\nOut[8]: 'Electronics'"
     ]
    }
   ],
   "source": [
    "#Q4.Identify the category with the highest total revenue.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum, col, avg\n",
    "from pyspark.sql.window import Window\n",
    "spark = SparkSession.builder.appName(\"Sales\").getOrCreate()\n",
    "data = [\n",
    "    (1, \"Product A\", \"Electronics\",500,100),\n",
    "    (2, \"Product B\", \"Clothing\",50,200),\n",
    "    (3, \"Product C\", \"Electronics\",800,50),\n",
    "    (4, \"Product D\", \"Beauty\",30,300),\n",
    "    (5, \"Product E\", \"Clothing\",75,150)\n",
    "]\n",
    "columns = [\"product_id\", \"product_name\", \"category\",\"price\",\"quantity_sold\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "category_revenue = df.withColumn('revenue',col('price')*col('quantity_sold')).groupBy('category').agg(sum('revenue').alias('total_revenue'))\n",
    "max_category_revenue = category_revenue.orderBy(desc('total_revenue')).first()\n",
    "max_category_revenue['category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77b83730-7822-4112-92c9-0da82095d851",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+-----------+------+\n|employee_id|  employee_name| department|salary|\n+-----------+---------------+-----------+------+\n|          1|       John Doe|Engineering| 90000|\n|          2|     Jane Smith|  Marketing| 75000|\n|          3|Michael Johnson|Engineering|105000|\n|          4|    Emily Davis|  Marketing| 80000|\n|          5|   Robert Brown|Engineering| 95000|\n|          6|   Linda Wilson|         HR| 60000|\n+-----------+---------------+-----------+------+\n\n+-------------+\n|total_payroll|\n+-------------+\n|       505000|\n+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total payroll cost for the company. Calculate the total number of employees in each department. Sample Dataset:\n",
    "# employee_id,employee_name,department,salary 1,John Doe,Engineering,90000 2,Jane Smith,Marketing,75000 3,Michael Johnson,Engineering, # 105000 4,Emily Davis,Marketing,80000 5,Robert Brown,Engineering,95000 6,Linda Wilson,HR,60000\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, avg, max, count\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"EmployeeAnalysis\").getOrCreate()\n",
    "\n",
    "# Load the dataset\n",
    "data = [\n",
    "    (1, \"John Doe\", \"Engineering\", 90000),\n",
    "    (2, \"Jane Smith\", \"Marketing\", 75000),\n",
    "    (3, \"Michael Johnson\", \"Engineering\", 105000),\n",
    "    (4, \"Emily Davis\", \"Marketing\", 80000),\n",
    "    (5, \"Robert Brown\", \"Engineering\", 95000),\n",
    "    (6, \"Linda Wilson\", \"HR\", 60000)\n",
    "]\n",
    "columns = [\"employee_id\", \"employee_name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "df.select(sum(df.salary).alias('total_payroll')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "129573ef-f865-4be3-be42-6774d5d30021",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+-----------+------+\n|employee_id|  employee_name| department|salary|\n+-----------+---------------+-----------+------+\n|          1|       John Doe|Engineering| 90000|\n|          2|     Jane Smith|  Marketing| 75000|\n|          3|Michael Johnson|Engineering|105000|\n|          4|    Emily Davis|  Marketing| 80000|\n|          5|   Robert Brown|Engineering| 95000|\n|          6|   Linda Wilson|         HR| 60000|\n+-----------+---------------+-----------+------+\n\n+-----------+------------------+\n| department|avg_salary_per_dep|\n+-----------+------------------+\n|Engineering| 96666.66666666667|\n|  Marketing|           77500.0|\n|         HR|           60000.0|\n+-----------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Find the average salary for each department.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, avg, max, count\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"EmployeeAnalysis\").getOrCreate()\n",
    "\n",
    "# Load the dataset\n",
    "data = [\n",
    "    (1, \"John Doe\", \"Engineering\", 90000),\n",
    "    (2, \"Jane Smith\", \"Marketing\", 75000),\n",
    "    (3, \"Michael Johnson\", \"Engineering\", 105000),\n",
    "    (4, \"Emily Davis\", \"Marketing\", 80000),\n",
    "    (5, \"Robert Brown\", \"Engineering\", 95000),\n",
    "    (6, \"Linda Wilson\", \"HR\", 60000)\n",
    "]\n",
    "columns = [\"employee_id\", \"employee_name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "df.groupBy('department').agg(avg('salary').alias('avg_salary_per_dep')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b297af80-dd97-4cdb-9e3d-d973e981f76d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+-----------+------+\n|employee_id|  employee_name| department|salary|\n+-----------+---------------+-----------+------+\n|          1|       John Doe|Engineering| 90000|\n|          2|     Jane Smith|  Marketing| 75000|\n|          3|Michael Johnson|Engineering|105000|\n|          4|    Emily Davis|  Marketing| 80000|\n|          5|   Robert Brown|Engineering| 95000|\n|          6|   Linda Wilson|         HR| 60000|\n+-----------+---------------+-----------+------+\n\n+-----------+---------------+-----------+------+-------+\n|employee_id|  employee_name| department|salary|row_num|\n+-----------+---------------+-----------+------+-------+\n|          3|Michael Johnson|Engineering|105000|      1|\n+-----------+---------------+-----------+------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "#Identify the highest-paid employee and their department.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import max, sum, avg, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"EmployeeAnalysis\").getOrCreate()\n",
    "\n",
    "# Load the dataset\n",
    "data = [\n",
    "    (1, \"John Doe\", \"Engineering\", 90000),\n",
    "    (2, \"Jane Smith\", \"Marketing\", 75000),\n",
    "    (3, \"Michael Johnson\", \"Engineering\", 105000),\n",
    "    (4, \"Emily Davis\", \"Marketing\", 80000),\n",
    "    (5, \"Robert Brown\", \"Engineering\", 95000),\n",
    "    (6, \"Linda Wilson\", \"HR\", 60000)\n",
    "]\n",
    "columns = [\"employee_id\", \"employee_name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "w = Window().orderBy(col('salary').desc())\n",
    "df = df.withColumn('row_num', row_number().over(w))\n",
    "# df.show()\n",
    "df.filter('row_num == 1').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "864a9bd2-b73e-4c84-9faf-646784aae96f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+------------+\n|order_id|customer_id|order_date|total_amount|\n+--------+-----------+----------+------------+\n|       1|       C101|2023-07-01|         150|\n|       2|       C102|2023-07-02|         200|\n|       3|       C101|2023-07-02|         100|\n|       4|       C103|2023-07-03|         300|\n|       5|       C102|2023-07-04|         250|\n|       6|       C101|2023-07-05|         120|\n+--------+-----------+----------+------------+\n\n+-------------+\n|total_revenue|\n+-------------+\n|         1120|\n+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "# PySpark Coding Challenge: Analyzing Online Store Orders\n",
    "\n",
    "# Task: You have a dataset containing information about orders from an online store. Your task is to use PySpark to analyze the data and answer a few questions using aggregate functions.\n",
    "\n",
    "# Dataset: The dataset is in CSV format and contains the following columns: order_id, customer_id, order_date, total_amount.\n",
    "\n",
    "# Questions:\n",
    "\n",
    "# Calculate the total revenue generated from all orders.\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, avg, max, count\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"OnlineStoreAnalysis\").getOrCreate()\n",
    "\n",
    "# Load the dataset\n",
    "data = [\n",
    "    (1, \"C101\", \"2023-07-01\", 150),\n",
    "    (2, \"C102\", \"2023-07-02\", 200),\n",
    "    (3, \"C101\", \"2023-07-02\", 100),\n",
    "    (4, \"C103\", \"2023-07-03\", 300),\n",
    "    (5, \"C102\", \"2023-07-04\", 250),\n",
    "    (6, \"C101\", \"2023-07-05\", 120)\n",
    "]\n",
    "columns = [\"order_id\", \"customer_id\", \"order_date\", \"total_amount\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "df.select(sum('total_amount').alias('total_revenue')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f38f8b8-3a0b-4654-8966-c4d76274151f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+------------+\n|order_id|customer_id|order_date|total_amount|\n+--------+-----------+----------+------------+\n|       1|       C101|2023-07-01|         150|\n|       2|       C102|2023-07-02|         200|\n|       3|       C101|2023-07-02|         100|\n|       4|       C103|2023-07-03|         300|\n|       5|       C102|2023-07-04|         250|\n|       6|       C101|2023-07-05|         120|\n+--------+-----------+----------+------------+\n\n+------------------+\n|     avg_order_amt|\n+------------------+\n|186.66666666666666|\n+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Find the average order amount.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, avg, max, count\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"OnlineStoreAnalysis\").getOrCreate()\n",
    "\n",
    "# Load the dataset\n",
    "data = [\n",
    "    (1, \"C101\", \"2023-07-01\", 150),\n",
    "    (2, \"C102\", \"2023-07-02\", 200),\n",
    "    (3, \"C101\", \"2023-07-02\", 100),\n",
    "    (4, \"C103\", \"2023-07-03\", 300),\n",
    "    (5, \"C102\", \"2023-07-04\", 250),\n",
    "    (6, \"C101\", \"2023-07-05\", 120)\n",
    "]\n",
    "columns = [\"order_id\", \"customer_id\", \"order_date\", \"total_amount\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "df.select(avg('total_amount').alias('avg_order_amt')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ead7735-6dc5-46f3-bfa8-840216e6f7b7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+------------+\n|order_id|customer_id|order_date|total_amount|\n+--------+-----------+----------+------------+\n|       1|       C101|2023-07-01|         150|\n|       2|       C102|2023-07-02|         200|\n|       3|       C101|2023-07-02|         100|\n|       4|       C103|2023-07-03|         300|\n|       5|       C102|2023-07-04|         250|\n|       6|       C101|2023-07-05|         120|\n+--------+-----------+----------+------------+\n\n+--------+-----------+----------+------------+-------+\n|order_id|customer_id|order_date|total_amount|row_num|\n+--------+-----------+----------+------------+-------+\n|       4|       C103|2023-07-03|         300|      1|\n+--------+-----------+----------+------------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "# Identify the highest total order amount and its corresponding customer.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, avg, max, count\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"OnlineStoreAnalysis\").getOrCreate()\n",
    "\n",
    "# Load the dataset\n",
    "data = [\n",
    "    (1, \"C101\", \"2023-07-01\", 150),\n",
    "    (2, \"C102\", \"2023-07-02\", 200),\n",
    "    (3, \"C101\", \"2023-07-02\", 100),\n",
    "    (4, \"C103\", \"2023-07-03\", 300),\n",
    "    (5, \"C102\", \"2023-07-04\", 250),\n",
    "    (6, \"C101\", \"2023-07-05\", 120)\n",
    "]\n",
    "columns = [\"order_id\", \"customer_id\", \"order_date\", \"total_amount\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "w = Window().orderBy(col('total_amount').desc())\n",
    "df = df.withColumn('row_num', row_number().over(w))\n",
    "# df.show()\n",
    "df.filter('row_num == 1').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8741b8b2-e1d4-4723-920a-90e49583f39c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+------------+\n|order_id|customer_id|order_date|total_amount|\n+--------+-----------+----------+------------+\n|       1|       C101|2023-07-01|         150|\n|       2|       C102|2023-07-02|         200|\n|       3|       C101|2023-07-02|         100|\n|       4|       C103|2023-07-03|         300|\n|       5|       C102|2023-07-04|         250|\n|       6|       C101|2023-07-05|         120|\n+--------+-----------+----------+------------+\n\n+-----------+---------------------+\n|customer_id|total_orders_per_cust|\n+-----------+---------------------+\n|       C101|                    3|\n|       C102|                    2|\n|       C103|                    1|\n+-----------+---------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total number of orders for each customer.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, avg, max, count\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"OnlineStoreAnalysis\").getOrCreate()\n",
    "\n",
    "# Load the dataset\n",
    "data = [\n",
    "    (1, \"C101\", \"2023-07-01\", 150),\n",
    "    (2, \"C102\", \"2023-07-02\", 200),\n",
    "    (3, \"C101\", \"2023-07-02\", 100),\n",
    "    (4, \"C103\", \"2023-07-03\", 300),\n",
    "    (5, \"C102\", \"2023-07-04\", 250),\n",
    "    (6, \"C101\", \"2023-07-05\", 120)\n",
    "]\n",
    "columns = [\"order_id\", \"customer_id\", \"order_date\", \"total_amount\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "df.groupBy('customer_id').agg(count('order_id').alias('total_orders_per_cust')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87aab78c-99fa-4d10-8ed9-3b5dc950db48",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-----+\n|student_id|subject|score|\n+----------+-------+-----+\n|         1|  Maths|   85|\n|         2|Science|   92|\n|         3|  Maths|   78|\n|         4|English|   88|\n|         5|Science|   95|\n|         6|  Maths|   90|\n+----------+-------+-----+\n\n+-------+-----------------+\n|subject|  avg_per_subject|\n+-------+-----------------+\n|  Maths|84.33333333333333|\n|Science|             93.5|\n|English|             88.0|\n+-------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# DAY 6: ğŸ’¡Task: You have a dataset containing student exam scores. Your task is to use PySpark to analyze the data and answer a few questions using aggregate functions.\n",
    "\n",
    "# ğŸ’¡Dataset: The dataset is in CSV format and contains the following columns: student_id, subject, score.\n",
    "\n",
    "# Sample Dataset :\n",
    "\n",
    "# student_id,subject,score 1,Math,85 2,Science,92 3,Math,78 4,English,88 5,Science,95 6,Math,90\n",
    "\n",
    "# ğŸ’¡ğ‚ğšğ¥ğœğ®ğ¥ğšğ­ğ ğ­ğ¡ğ ğšğ¯ğğ«ğšğ ğ ğ¬ğœğ¨ğ«ğ ğŸğ¨ğ« ğğšğœğ¡ ğ¬ğ®ğ›ğ£ğğœğ­.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, avg, max, count\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"StudentExamScores\").getOrCreate()\n",
    "\n",
    "# Load the dataset\n",
    "data = [\n",
    "    (1, \"Maths\", 85),\n",
    "    (2, \"Science\", 92),\n",
    "    (3, \"Maths\", 78),\n",
    "    (4, \"English\", 88),\n",
    "    (5, \"Science\", 95),\n",
    "    (6, \"Maths\", 90)\n",
    "]\n",
    "columns = [\"student_id\", \"subject\", \"score\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "df.groupBy('subject').agg(avg('score').alias('avg_per_subject')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "234f2841-6ec7-4b12-8d1c-0f4c63b37185",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-----+\n|student_id|subject|score|\n+----------+-------+-----+\n|         1|  Maths|   85|\n|         2|Science|   92|\n|         3|  Maths|   78|\n|         4|English|   88|\n|         5|Science|   95|\n|         6|  Maths|   90|\n+----------+-------+-----+\n\n+-------+----------+-----+-------------+\n|subject|student_id|score|highest_score|\n+-------+----------+-----+-------------+\n|English|         4|   88|           88|\n|Science|         5|   95|           95|\n|  Maths|         6|   90|           90|\n+-------+----------+-----+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "# ğˆğğğ§ğ­ğ¢ğŸğ² ğ­ğ¡ğ ğ¡ğ¢ğ ğ¡ğğ¬ğ­ ğ¬ğœğ¨ğ«ğ ğšğ§ğ ğ¢ğ­ğ¬ ğœğ¨ğ«ğ«ğğ¬ğ©ğ¨ğ§ğğ¢ğ§ğ  ğ¬ğ­ğ®ğğğ§ğ­ ğŸğ¨ğ« ğğšğœğ¡ ğ¬ğ®ğ›ğ£ğğœğ­.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"StudentExamScores\").getOrCreate()\n",
    "\n",
    "# Load the dataset\n",
    "data = [\n",
    "    (1, \"Maths\", 85),\n",
    "    (2, \"Science\", 92),\n",
    "    (3, \"Maths\", 78),\n",
    "    (4, \"English\", 88),\n",
    "    (5, \"Science\", 95),\n",
    "    (6, \"Maths\", 90)\n",
    "]\n",
    "columns = [\"student_id\", \"subject\", \"score\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "highest_score_per_subject = df.groupBy(\"subject\").agg(max(\"score\").alias(\"highest_score\"))\n",
    "# highest_score_per_subject.show()\n",
    "highest_score_students = df.join(highest_score_per_subject, on=\"subject\").filter(col(\"score\") == col(\"highest_score\"))\n",
    "highest_score_students.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c2de1d2-57df-4810-98fb-e3ec71ca06f8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-----+\n|student_id|subject|score|\n+----------+-------+-----+\n|         1|  Maths|   85|\n|         2|Science|   92|\n|         3|  Maths|   78|\n|         4|English|   88|\n|         5|Science|   95|\n|         6|  Maths|   90|\n+----------+-------+-----+\n\n+-------+----------------------+\n|subject|total_students_per_sub|\n+-------+----------------------+\n|  Maths|                     3|\n|Science|                     2|\n|English|                     1|\n+-------+----------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# ğ‚ğšğ¥ğœğ®ğ¥ğšğ­ğ ğ­ğ¡ğ ğ­ğ¨ğ­ğšğ¥ ğ§ğ®ğ¦ğ›ğğ« ğ¨ğŸ ğ¬ğ­ğ®ğğğ§ğ­ğ¬ ğ°ğ¡ğ¨ ğ­ğ¨ğ¨ğ¤ ğğšğœğ¡ ğ¬ğ®ğ›ğ£ğğœğ­.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, avg, max, count\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"StudentExamScores\").getOrCreate()\n",
    "\n",
    "# Load the dataset\n",
    "data = [\n",
    "    (1, \"Maths\", 85),\n",
    "    (2, \"Science\", 92),\n",
    "    (3, \"Maths\", 78),\n",
    "    (4, \"English\", 88),\n",
    "    (5, \"Science\", 95),\n",
    "    (6, \"Maths\", 90)\n",
    "]\n",
    "columns = [\"student_id\", \"subject\", \"score\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "df.groupBy('subject').agg(count('student_id').alias('total_students_per_sub')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12467514-44bc-478a-8a9c-6435cbe3b20c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-----+\n|student_id|subject|score|\n+----------+-------+-----+\n|         1|  Maths|   85|\n|         2|Science|   92|\n|         3|  Maths|   78|\n|         4|English|   88|\n|         5|Science|   95|\n|         6|  Maths|   90|\n+----------+-------+-----+\n\nOut[19]: 'Science'"
     ]
    }
   ],
   "source": [
    "# ğŸ’¡ğ…ğ¢ğ§ğ ğ­ğ¡ğ ğ¬ğ®ğ›ğ£ğğœğ­(ğ¬) ğ°ğ¢ğ­ğ¡ ğ­ğ¡ğ ğ¡ğ¢ğ ğ¡ğğ¬ğ­ ğšğ¯ğğ«ğšğ ğ ğ¬ğœğ¨ğ«ğ.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, avg, max, count\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"StudentExamScores\").getOrCreate()\n",
    "\n",
    "# Load the dataset\n",
    "data = [\n",
    "    (1, \"Maths\", 85),\n",
    "    (2, \"Science\", 92),\n",
    "    (3, \"Maths\", 78),\n",
    "    (4, \"English\", 88),\n",
    "    (5, \"Science\", 95),\n",
    "    (6, \"Maths\", 90)\n",
    "]\n",
    "columns = [\"student_id\", \"subject\", \"score\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "df = df.groupBy('subject').agg(avg('score').alias('avg_per_subject'))\n",
    "max_avg = df.orderBy(col('avg_per_subject').desc()).first()\n",
    "max_avg['subject']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c65873ba-1101-4afc-8edd-8629b9fd9da4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n|user_id|          timestamp|\n+-------+-------------------+\n|  user1|2023-08-21 10:00:00|\n|  user2|2023-08-21 11:30:00|\n|  user1|2023-08-21 12:15:00|\n|  user3|2023-08-21 13:45:00|\n|  user2|2023-08-21 14:30:00|\n|  user1|2023-08-21 15:00:00|\n+-------+-------------------+\n\n+-------------------+\n| earliest_timestamp|\n+-------------------+\n|2023-08-21 10:00:00|\n+-------------------+\n\n+-------------------+\n|   latest_timestamp|\n+-------------------+\n|2023-08-21 15:00:00|\n+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Questions:1 Sample Dataset :\n",
    "\n",
    "# columns = [\"user_id\", \"timestamp\"]\n",
    "\n",
    "# data= [(\"user1\", \"2023-08-21 10:00:00\"), (\"user2\", \"2023-08-21 11:30:00\"), (\"user1\", \"2023-08-21 12:15:00\"), (\"user3\", \"2023-08-21 13:45:00\"), (\"user2\", \"2023-08-21 14:30:00\"), (\"user1\", \"2023-08-21 15:00:00\")]\n",
    "\n",
    "# ğŸ’¡ ğ…ğ¢ğ§ğ ğ­ğ¡ğ ğğšğ«ğ¥ğ¢ğğ¬ğ­ ğšğ§ğ ğ¥ğšğ­ğğ¬ğ­ ğ­ğ¢ğ¦ğğ¬ğ­ğšğ¦ğ©ğ¬ ğ¢ğ§ ğ­ğ¡ğ ğğšğ­ğšğ¬ğğ­.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, to_timestamp, min, max\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"StudentExamScores\").getOrCreate()\n",
    "\n",
    "# Load the dataset\n",
    "data= [\n",
    "    (\"user1\", \"2023-08-21 10:00:00\"),\n",
    "    (\"user2\", \"2023-08-21 11:30:00\"),\n",
    "    (\"user1\", \"2023-08-21 12:15:00\"),\n",
    "    (\"user3\", \"2023-08-21 13:45:00\"),\n",
    "    (\"user2\", \"2023-08-21 14:30:00\"),\n",
    "    (\"user1\", \"2023-08-21 15:00:00\")\n",
    "    ]\n",
    "columns = [\"user_id\", \"timestamp\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df = df.withColumn('timestamp', to_timestamp(col('timestamp')))\n",
    "df.show()\n",
    "earliest_timestamp = df.select(min('timestamp').alias('earliest_timestamp'))\n",
    "earliest_timestamp.show()\n",
    "latest_timestamp = df.select(max('timestamp').alias('latest_timestamp'))\n",
    "latest_timestamp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb62c746-1fdc-4b21-ade4-c7cf29ad7a67",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n|user_id|          timestamp|\n+-------+-------------------+\n|  user1|2023-08-21 10:00:00|\n|  user2|2023-08-21 11:30:00|\n|  user1|2023-08-21 12:15:00|\n|  user3|2023-08-21 13:45:00|\n|  user2|2023-08-21 14:30:00|\n|  user1|2023-08-21 15:00:00|\n+-------+-------------------+\n\n+-------+----------------+\n|user_id|no_of_activities|\n+-------+----------------+\n|  user1|               3|\n|  user2|               2|\n|  user3|               1|\n+-------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# ğŸ’¡ ğ‚ğ¨ğ®ğ§ğ­ ğ­ğ¡ğ ğ§ğ®ğ¦ğ›ğğ« ğ¨ğŸ ğšğœğ­ğ¢ğ¯ğ¢ğ­ğ¢ğğ¬ ğ©ğğ« ğ®ğ¬ğğ«.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, avg, max, count\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"StudentExamScores\").getOrCreate()\n",
    "\n",
    "# Load the dataset\n",
    "data= [\n",
    "    (\"user1\", \"2023-08-21 10:00:00\"),\n",
    "    (\"user2\", \"2023-08-21 11:30:00\"),\n",
    "    (\"user1\", \"2023-08-21 12:15:00\"),\n",
    "    (\"user3\", \"2023-08-21 13:45:00\"),\n",
    "    (\"user2\", \"2023-08-21 14:30:00\"),\n",
    "    (\"user1\", \"2023-08-21 15:00:00\")\n",
    "    ]\n",
    "columns = [\"user_id\", \"timestamp\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df = df.withColumn('timestamp', to_timestamp(col('timestamp')))\n",
    "df.show()\n",
    "df.groupBy('user_id').agg(count('timestamp').alias('no_of_activities')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77601ec5-8cc2-44ba-b197-17d5e7a18e6c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n|user_id|          timestamp|\n+-------+-------------------+\n|  user1|2023-08-21 10:00:00|\n|  user2|2023-08-21 11:30:00|\n|  user1|2023-08-21 12:15:00|\n|  user3|2023-08-21 13:45:00|\n|  user2|2023-08-21 14:30:00|\n|  user1|2023-08-21 15:00:00|\n+-------+-------------------+\n\n+-------+-------------------+-------------------+\n|user_id|          timestamp|     prev_timestamp|\n+-------+-------------------+-------------------+\n|  user1|2023-08-21 10:00:00|               null|\n|  user1|2023-08-21 12:15:00|2023-08-21 10:00:00|\n|  user1|2023-08-21 15:00:00|2023-08-21 12:15:00|\n|  user2|2023-08-21 11:30:00|               null|\n|  user2|2023-08-21 14:30:00|2023-08-21 11:30:00|\n|  user3|2023-08-21 13:45:00|               null|\n+-------+-------------------+-------------------+\n\n+-------+-------------------+-------------------+-------------+\n|user_id|          timestamp|     prev_timestamp|time_duration|\n+-------+-------------------+-------------------+-------------+\n|  user1|2023-08-21 10:00:00|               null|         null|\n|  user1|2023-08-21 12:15:00|2023-08-21 10:00:00|         8100|\n|  user1|2023-08-21 15:00:00|2023-08-21 12:15:00|         9900|\n|  user2|2023-08-21 11:30:00|               null|         null|\n|  user2|2023-08-21 14:30:00|2023-08-21 11:30:00|        10800|\n|  user3|2023-08-21 13:45:00|               null|         null|\n+-------+-------------------+-------------------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "# ğŸ’¡ğ‚ğšğ¥ğœğ®ğ¥ğšğ­ğ ğ­ğ¡ğ ğ­ğ¢ğ¦ğ ğğ®ğ«ğšğ­ğ¢ğ¨ğ§ ğ›ğğ­ğ°ğğğ§ ğœğ¨ğ§ğ¬ğğœğ®ğ­ğ¢ğ¯ğ ğšğœğ­ğ¢ğ¯ğ¢ğ­ğ¢ğğ¬ ğŸğ¨ğ« ğğšğœğ¡ ğ®ğ¬ğğ«.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, avg, max, count, lag, cast\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"StudentExamScores\").getOrCreate()\n",
    "\n",
    "# Load the dataset\n",
    "data= [\n",
    "    (\"user1\", \"2023-08-21 10:00:00\"),\n",
    "    (\"user2\", \"2023-08-21 11:30:00\"),\n",
    "    (\"user1\", \"2023-08-21 12:15:00\"),\n",
    "    (\"user3\", \"2023-08-21 13:45:00\"),\n",
    "    (\"user2\", \"2023-08-21 14:30:00\"),\n",
    "    (\"user1\", \"2023-08-21 15:00:00\")\n",
    "    ]\n",
    "columns = [\"user_id\", \"timestamp\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df = df.withColumn('timestamp', to_timestamp(col('timestamp')))\n",
    "df.show()\n",
    "# print(df.dtypes)\n",
    "# print(df.schema)\n",
    "w = Window().partitionBy('user_id').orderBy('timestamp')\n",
    "df_with_prev_timestamp = df.withColumn('prev_timestamp',lag('timestamp').over(w))\n",
    "df_with_prev_timestamp.show()\n",
    "df_with_prev_timestamp.withColumn('time_duration',col('timestamp').cast('long')- col('prev_timestamp').cast('long')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1759baf9-a2fd-4ea9-8146-75c8c41e95c9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------------------+\n|user_id|action  |timestamp          |\n+-------+--------+-------------------+\n|1      |login   |2023-08-20 10:23:45|\n|2      |view    |2023-08-20 11:15:30|\n|1      |purchase|2023-08-20 12:45:18|\n|3      |view    |2023-08-20 13:30:22|\n+-------+--------+-------------------+\n\n+-------------+\n|count(action)|\n+-------------+\n|            4|\n+-------------+\n\n+--------+\n|  action|\n+--------+\n|   login|\n|    view|\n|purchase|\n+--------+\n\n+-------+--------+-------------------+-------------------+\n|user_id|  action|          timestamp|     prev_timestamp|\n+-------+--------+-------------------+-------------------+\n|      1|   login|2023-08-20 10:23:45|               null|\n|      1|purchase|2023-08-20 12:45:18|2023-08-20 10:23:45|\n|      2|    view|2023-08-20 11:15:30|               null|\n|      3|    view|2023-08-20 13:30:22|               null|\n+-------+--------+-------------------+-------------------+\n\n+-------+--------+-------------------+-------------------+-------------+\n|user_id|  action|          timestamp|     prev_timestamp|time_duration|\n+-------+--------+-------------------+-------------------+-------------+\n|      1|   login|2023-08-20 10:23:45|               null|         null|\n|      1|purchase|2023-08-20 12:45:18|2023-08-20 10:23:45|         8493|\n|      2|    view|2023-08-20 11:15:30|               null|         null|\n|      3|    view|2023-08-20 13:30:22|               null|         null|\n+-------+--------+-------------------+-------------------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Questions:2\n",
    "\n",
    "# Count the number of action\n",
    "# 2: What are the unique actions recorded in the dataset?\n",
    "# 3: Calculate the time duration between consecutive activities for each user\n",
    "# data = [ (1, \"login\", \"2023-08-20 10:23:45\"), (2, \"view\", \"2023-08-20 11:15:30\"), (1, \"purchase\", \"2023-08-20 12:45:18\"), (3, \"view\", \"2023-08-20 13:30:22\") ] columns = [\"user_id\", \"action\", \"timestamp\"]\n",
    "\n",
    "#import relevant libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType,FloatType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank,col,min,max,row_number,max,asc,desc,when,count,sum\n",
    "#Let's create Sparksession First\n",
    "\n",
    "\n",
    "spark=SparkSession.builder.master(\"local[1]\").appName(\"Actions\").getOrCreate()\n",
    "\n",
    "data = [\n",
    "    (1, \"login\", \"2023-08-20 10:23:45\"),\n",
    "    (2, \"view\", \"2023-08-20 11:15:30\"),\n",
    "    (1, \"purchase\", \"2023-08-20 12:45:18\"),\n",
    "    (3, \"view\", \"2023-08-20 13:30:22\")\n",
    "]\n",
    "columns = [\"user_id\", \"action\", \"timestamp\"]\n",
    "#Schema\n",
    "\n",
    "columns = StructType([StructField(\"user_id\",StringType()), StructField(\"action\",StringType()),StructField(\"timestamp\", StringType())])\n",
    "action_df = spark.createDataFrame(data, columns)\n",
    "action_df = action_df.withColumn('timestamp', to_timestamp(col('timestamp')))\n",
    "action_df.show(truncate=False)\n",
    "\n",
    "#1\n",
    "action_df.select(count('action')).show()\n",
    "#2\n",
    "action_df.select(col('action')).distinct().show()\n",
    "#3\n",
    "w = Window().partitionBy('user_id').orderBy('timestamp')\n",
    "df_with_prev_timestamp = action_df.withColumn('prev_timestamp',lag('timestamp').over(w))\n",
    "df_with_prev_timestamp.show()\n",
    "df_with_prev_timestamp.withColumn('time_duration',col('timestamp').cast('long')- col('prev_timestamp').cast('long')).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22f15879-de82-437c-ace2-2347d41a4ed1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+------+\n|name  |dept_name|Salary|\n+------+---------+------+\n|James |Sales    |2000  |\n|sofy  |Sales    |3000  |\n|Laren |Sales    |4000  |\n|Kiku  |Sales    |5000  |\n|Sam   |Finance  |6000  |\n|Samuel|Finance  |7000  |\n|Yash  |Finance  |8000  |\n|Rabin |Finance  |9000  |\n|Lukasz|Marketing|10000 |\n|Jolly |Marketing|11000 |\n|Mausam|Marketing|12000 |\n|Lamba |Marketing|13000 |\n|Jogesh|HR       |14000 |\n|Mannu |HR       |15000 |\n|Sylvia|HR       |16000 |\n|Sama  |HR       |17000 |\n+------+---------+------+\n\n+------+---------+------+---------+\n|  name|dept_name|Salary|cumu_dist|\n+------+---------+------+---------+\n|   Sam|  Finance|  6000|     0.25|\n|Samuel|  Finance|  7000|      0.5|\n|  Yash|  Finance|  8000|     0.75|\n| Rabin|  Finance|  9000|      1.0|\n|Jogesh|       HR| 14000|     0.25|\n| Mannu|       HR| 15000|      0.5|\n|Sylvia|       HR| 16000|     0.75|\n|  Sama|       HR| 17000|      1.0|\n|Lukasz|Marketing| 10000|     0.25|\n| Jolly|Marketing| 11000|      0.5|\n|Mausam|Marketing| 12000|     0.75|\n| Lamba|Marketing| 13000|      1.0|\n| James|    Sales|  2000|     0.25|\n|  sofy|    Sales|  3000|      0.5|\n| Laren|    Sales|  4000|     0.75|\n|  Kiku|    Sales|  5000|      1.0|\n+------+---------+------+---------+\n\n+---------+----------+----------+\n|dept_name|min_salary|max_salary|\n+---------+----------+----------+\n|Finance  |6000      |9000      |\n|HR       |14000     |17000     |\n|Marketing|10000     |13000     |\n|Sales    |2000      |5000      |\n+---------+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Q2 . Find out min, max and cumulative salary in a dataset with emp_name, dept_name and salary as columns\n",
    "\n",
    "# some interviewer will specifically ask you to Create a sparksession, dataframe , define schema and then write the logic.\n",
    "\n",
    "# (Asked in Service based companies)\n",
    "\n",
    "# Best Answer \n",
    "\n",
    "#import relevant libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType,FloatType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank,col,min,max,row_number,max,asc,desc,when,count,sum, cume_dist\n",
    "#Let's create Sparksession First\n",
    "\n",
    "\n",
    "spark=SparkSession.builder.master(\"local[1]\").appName(\"Prep\").getOrCreate()\n",
    "\n",
    "data= [(\"James\", \"Sales\", 2000),\n",
    "(\"sofy\", \"Sales\", 3000),\n",
    "(\"Laren\", \"Sales\", 4000),\n",
    "(\"Kiku\", \"Sales\", 5000),\n",
    "(\"Sam\", \"Finance\", 6000),\n",
    "(\"Samuel\", \"Finance\", 7000),\n",
    "(\"Yash\", \"Finance\", 8000),\n",
    "(\"Rabin\", \"Finance\", 9000),\n",
    "(\"Lukasz\", \"Marketing\", 10000),\n",
    "(\"Jolly\", \"Marketing\", 11000),\n",
    "(\"Mausam\", \"Marketing\", 12000),\n",
    "(\"Lamba\", \"Marketing\", 13000),\n",
    "(\"Jogesh\", \"HR\", 14000),\n",
    "(\"Mannu\", \"HR\", 15000),\n",
    "(\"Sylvia\", \"HR\", 16000),\n",
    "(\"Sama\", \"HR\", 17000),\n",
    "]\n",
    "\n",
    "#Schema\n",
    "\n",
    "emp_schema = StructType([StructField(\"name\",StringType()), StructField(\"dept_name\",StringType()),StructField(\"Salary\", StringType())])\n",
    "employees_Salary_df = spark.createDataFrame(data, emp_schema)\n",
    "employees_Salary_df.show(truncate=False)\n",
    "\n",
    "#creating window\n",
    "w = Window().partitionBy('dept_name').orderBy('Salary')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#getting cumulative salary\n",
    "employees_Salary_df.withColumn('cumu_dist',cume_dist().over(w)).show()\n",
    "\n",
    "\n",
    "#getting min and max salary\n",
    "employees_Salary_df.groupBy('dept_name').agg(min('Salary').alias('min_salary'),max('Salary').alias('max_salary')).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8793953a-f732-40c3-8af0-974287232543",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "ğŸŒŸ Pyspark scenario questions : ğŸ¤·â€â™‚ï¸\n",
    "==================\n",
    "ğŸ‘‰ Here in this question we used LAG(), datediff() and Aggregate MAX() functions.\n",
    "\n",
    "âœ” LAG() function is a window function that returns the value that is offset rows before the current row, and defaults if there are less than offset rows before the current row. This is equivalent to the LAG function in SQL. The PySpark Window functions operate on a group of rows (like frame, partition) and return a single value for every input row.\n",
    "\n",
    "Imagine a vast customer interaction dataset, and you want to understand the maximum time gap between consecutive interactions for each customer. This intricate task can be efficiently accomplished using PySpark's LAG function in conjunction with the Aggregate MAX function.\n",
    "\n",
    "ğŸ”… Challenge:\n",
    "------------\n",
    "Calculate the longest time duration between two consecutive customer interactions using the PySpark LAG function. Moreover, you want to find the maximum of these time gaps using the Aggregate MAX function. This intricate analysis can offer valuable insights into customer engagement patterns.\n",
    "\n",
    "ğŸ”‘ Solution :\n",
    "----------\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ComplexFunctionExample\").getOrCreate()\n",
    "\n",
    "hashtag#sample  customer interaction data\n",
    "data = [(\"Cust1\", \"2023-07-01\", \"Interaction\"),\n",
    " (\"Cust1\", \"2023-07-03\", \"Interaction\"),\n",
    " (\"Cust1\", \"2023-07-05\", \"Interaction\"),\n",
    " (\"Cust2\", \"2023-07-02\", \"Interaction\"),\n",
    " (\"Cust2\", \"2023-07-06\", \"Interaction\")]\n",
    "\n",
    "hashtag#create  Dataframe\n",
    "columns = [\"customer_id\",\"interaction_date\",\"interaction_type\"]\n",
    "df = spark.createDataFrame(data,columns)\n",
    "\n",
    "\n",
    "output:\n",
    "=====\n",
    "+-----------+------------+\n",
    "|customer_id|max_time_gap|\n",
    "+-----------+------------+\n",
    "|   Cust2|      4|\n",
    "|   Cust1|      2|\n",
    "+-----------+------------+\n",
    "\n",
    "ğŸ“ˆ Logic :- we used LAG() function to compare the interaction of previous customer at specified window, used datediff() to get the difference between interactions, Aggregation MAX() to pull out the maximum time gap.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f84e4fb0-b27f-451e-a617-7dc6192c95d7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+----------------+\n|customer_id|interaction_date|interaction_type|\n+-----------+----------------+----------------+\n|Cust1      |2023-07-01      |Interaction     |\n|Cust1      |2023-07-03      |Interaction     |\n|Cust1      |2023-07-05      |Interaction     |\n|Cust2      |2023-07-02      |Interaction     |\n|Cust2      |2023-07-06      |Interaction     |\n+-----------+----------------+----------------+\n\n+-----------+----------------+\n|customer_id|maximum time gap|\n+-----------+----------------+\n|      Cust1|               2|\n|      Cust2|               4|\n+-----------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType,FloatType\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName(\"ComplexFunctionExample\").getOrCreate()\n",
    "\n",
    "#sample  customer interaction data\n",
    "data = [(\"Cust1\", \"2023-07-01\", \"Interaction\"),\n",
    " (\"Cust1\", \"2023-07-03\", \"Interaction\"),\n",
    " (\"Cust1\", \"2023-07-05\", \"Interaction\"),\n",
    " (\"Cust2\", \"2023-07-02\", \"Interaction\"),\n",
    " (\"Cust2\", \"2023-07-06\", \"Interaction\")]\n",
    "\n",
    "#create  Dataframe\n",
    "\n",
    "schema = StructType([StructField(\"customer_id\",StringType()), StructField(\"interaction_date\",StringType()),StructField(\"interaction_type\", StringType())])\n",
    "employees_Salary_df = spark.createDataFrame(data, schema)\n",
    "employees_Salary_df.show(truncate=False)\n",
    "\n",
    "w = Window().partitionBy('customer_id').orderBy('interaction_date')\n",
    "df_prev_date = employees_Salary_df.withColumn('prev_date',F.lag('interaction_date').over(w))\n",
    "df_date_diff = df_prev_date.withColumn('diff_date',F.datediff(col('interaction_date'),col('prev_date')))\n",
    "df_date_diff.groupBy('customer_id').agg(max('diff_date').alias('maximum time gap')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "697a565f-66a6-4cf0-9361-6765274c6db9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+------------+------------+-----------+-----------+\n|sender_id|send_to_id|request_date|requester_id|accepter_id|accept_date|\n+---------+----------+------------+------------+-----------+-----------+\n|        1|         2|  2016/06/01|           1|          2| 2016/06/03|\n|        1|         3|  2016/06/01|           1|          3| 2016/06/08|\n|        1|         3|  2016/06/01|           1|          3| 2016/06/08|\n+---------+----------+------------+------------+-----------+-----------+\n\ntotal_requests : 3\nunique_requests : 2\n+-------------+---------------+----------------+\n|total_request|unique_requests|accepatance_rate|\n+-------------+---------------+----------------+\n|            3|              2|            0.67|\n+-------------+---------------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Write a Pyspark query to find the overall acceptance rate of requests, which is the number of acceptance divided by the number of requests. Return the answer rounded to 2 decimals places. If there are duplicate requests consider them only.If there are no requests at all, you should return 0.00 as the accept_rate.\n",
    "\n",
    "# FriendRequest table: +-----------+------------+--------------+ | sender_id | send_to_id | request_date | +-----------+------------+--------------+ | 1 | 2 | 2016/06/01 | | 1 | 3 | 2016/06/01 | | 1 | 3 | 2016/06/01 | +-----------+------------+--------------+\n",
    "\n",
    "# RequestAccepted table: +--------------+-------------+-------------+ | requester_id | accepter_id | accept_date | +--------------+-------------+-------------+ | 1 | 2 | 2016/06/03 | | 1 | 3 | 2016/06/08 | +--------------+-------------+-------------+\n",
    "\n",
    "# Result table: +-------------+ | unique_accepted_request | +-------------+ | 2 | +-------------+\n",
    "\n",
    "# +-------------+ | total_request | +-------------+ | 3 | +-------------+\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, avg, max, count\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"FriendRequest\").getOrCreate()\n",
    "spark1 = SparkSession.builder.appName(\"RequestAccepted\").getOrCreate()\n",
    "\n",
    "# Load the dataset\n",
    "data = [\n",
    "    (1, 2, \"2016/06/01\"),\n",
    "    (1, 3, \"2016/06/01\"),\n",
    "    (1, 3, \"2016/06/01\")\n",
    "]\n",
    "data1 = [\n",
    "    (1, 2, \"2016/06/03\"),\n",
    "    (1, 3, \"2016/06/08\")\n",
    "]\n",
    "columns = [\"sender_id\", \"send_to_id\", \"request_date\"]\n",
    "columns1 = [\"requester_id\", \"accepter_id\", \"accept_date\"]\n",
    "df_FR = spark.createDataFrame(data, columns)\n",
    "df_RA = spark.createDataFrame(data1, columns1)\n",
    "# df_FR.show()\n",
    "# df_RA.show()\n",
    "\n",
    "# Step 01: Join data frames based on conditions\n",
    "joined_df = df_FR.join(df_RA, (df_FR.sender_id == df_RA.requester_id) & (df_FR.send_to_id == df_RA.accepter_id),'inner')\n",
    "joined_df.show()\n",
    "# Step 02: Total requests and unique accepted requests\n",
    "total_request = joined_df.count()\n",
    "print('total_requests :', total_request)\n",
    "unique_requests = joined_df.select('requester_id','accepter_id').distinct().count()\n",
    "print('unique_requests :',unique_requests)\n",
    "# Step 03: Calculate the acceptance rate\n",
    "accepatance_rate = 0 if total_request == 0 else round(unique_requests/total_request,2)\n",
    "# Step 04: Show the result\n",
    "result_df = spark.createDataFrame([(total_request, unique_requests, accepatance_rate)],['total_request', 'unique_requests', 'accepatance_rate'])\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "448f7917-22b4-4b4f-a562-2d861db00104",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Every Advance Function\n",
    "You have a dataset containing user activity logs in a PySpark DataFrame with the following columns: user_id, timestamp, and action. The action column indicates whether the user started or ended a session. It can have values 'start' or 'end'. Your task is to calculate the average duration of user sessions.\n",
    "\n",
    "data = [ (1, \"2022-01-01 10:00\", \"start\"), (1, \"2022-01-01 10:15\", \"end\"), (2, \"2022-01-01 11:00\", \"start\"), (1, \"2022-01-01 11:30\", \"start\"), (2, \"2022-01-01 11:45\", \"end\"), (1, \"2022-01-01 12:00\", \"end\") ] schema = [\"user_id\", \"timestamp\", \"action\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccb6b208-4743-4ba7-a8b7-47379f6e593c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "NTILE() window function returns the relative rank of result rows within a window partition. In below example we have used 2 as an argument to ntile hence it returns ranking between 2 values (1 and 2)\n",
    "\n",
    "RANK() window function is used to provide a rank to the result within a window partition. This function leaves gaps in rank when there are ties.\n",
    "\n",
    "DENSE_RANK() window function is used to get the result with rank of rows within a window partition without any gaps. This is similar to RANK() function difference being rank function leaves gaps in rank when there are ties.\n",
    "\n",
    "ROW_NUMBER() window function is used to give the sequential row number starting from 1 to the result of each window partition.\n",
    "\n",
    "LAG() is a function that works as the offset row returning the value of the before row of a column with respect to the current row.\n",
    "\n",
    "LEAD() is a function that works as the offset row returning the value of the after row of a column with respect to the current row.\n",
    "\n",
    "PERCENTILE_RANK() Returns the percentile rank of rows within a window partition.\n",
    "\n",
    "Here is the Code ğŸ‘‡ :\n",
    "--------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0593ca87-a48b-4442-a4c7-add88275eadf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------+----------+----+----------+------------+----+----+-----+\n|   name|     dept|salary|row_number|rank|dense_rank|percent_rank| lag|lead|ntile|\n+-------+---------+------+----------+----+----------+------------+----+----+-----+\n|  Maria|  Finance|  3000|         1|   1|         1|         0.0|null|3900|    1|\n|  Scott|  Finance|  3300|         2|   2|         2|         0.5|null|null|    1|\n|    Jen|  Finance|  3900|         3|   3|         3|         1.0|3000|null|    2|\n|  Kumar|Marketing|  2000|         1|   1|         1|         0.0|null|null|    1|\n|   Jeff|Marketing|  3000|         2|   2|         2|         1.0|null|null|    2|\n|  James|    Sales|  3000|         1|   1|         1|         0.0|null|4100|    1|\n|  James|    Sales|  3000|         2|   1|         1|         0.0|null|4100|    1|\n| Robert|    Sales|  4100|         3|   3|         2|         0.5|3000|4600|    1|\n|   Saif|    Sales|  4100|         4|   3|         2|         0.5|3000|null|    2|\n|Michael|    Sales|  4600|         5|   5|         3|         1.0|4100|null|    2|\n+-------+---------+------+----------+----+----------+------------+----+----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank, row_number, dense_rank, lag, lead, percent_rank, ntile\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Window_functions\").getOrCreate()\n",
    "\n",
    "simpleData = ((\"James\", \"Sales\", 3000),\n",
    " (\"Michael\", \"Sales\", 4600),\n",
    " (\"Robert\", \"Sales\", 4100),\n",
    " (\"Maria\", \"Finance\", 3000),\n",
    " (\"James\", \"Sales\", 3000),\n",
    " (\"Scott\", \"Finance\", 3300),\n",
    " (\"Jen\", \"Finance\", 3900),\n",
    " (\"Jeff\", \"Marketing\", 3000),\n",
    " (\"Kumar\", \"Marketing\", 2000),\n",
    " (\"Saif\", \"Sales\", 4100)\n",
    " )\n",
    "columns = (\"name\",\"dept\",\"salary\")\n",
    "\n",
    "#creating a dataframe\n",
    "df = spark.createDataFrame(simpleData,columns)\n",
    "\n",
    "#creating a Window specification\n",
    "window_spec = Window.partitionBy(\"dept\").orderBy(\"salary\")\n",
    "\n",
    "#Applying window functions\n",
    "df.withColumn(\"row_number\",row_number().over(window_spec))\\\n",
    " .withColumn(\"rank\",rank().over(window_spec))\\\n",
    " .withColumn(\"dense_rank\",dense_rank().over(window_spec))\\\n",
    " .withColumn(\"percent_rank\",percent_rank().over(window_spec))\\\n",
    " .withColumn(\"lag\",lag(\"salary\",2).over(window_spec))\\\n",
    " .withColumn(\"lead\",lead(\"salary\",2).over(window_spec))\\\n",
    " .withColumn(\"ntile\",ntile(2).over(window_spec))\\\n",
    " .show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Pyspark 1",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
