{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8fef7d6-6b4e-4ca2-884b-364a0eb3a06b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Write a pyspark dataframe query to find all duplicate emails in a table named Person.\n",
    "\n",
    "+----+---------+ | Id | Email | +----+---------+ | 1 | a@b.com | | 2 | c@d.com | | 3 | a@b.com | +----+---------+ For example, your query should return the following for the above table:\n",
    "\n",
    "+---------+ | Email | +---------+ | a@b.com | +---------+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bef09362-3c20-4049-b52d-98a751c4f693",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a70d66a-8e8d-4a92-afbb-4b9eae256566",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n| Id|  Email|\n+---+-------+\n|  1|a@b.com|\n|  2|c@d.com|\n|  3|a@b.com|\n+---+-------+\n\n+-------+\n|  Email|\n+-------+\n|a@b.com|\n+-------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import count\n",
    "spark = SparkSession.builder.appName(\"Person\").getOrCreate()\n",
    "data = [\n",
    "    (1, \"a@b.com\"),\n",
    "    (2, \"c@d.com\"),\n",
    "    (3, \"a@b.com\")\n",
    "]\n",
    "columns = [\"Id\", \"Email\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "\n",
    "\n",
    "# Find duplicate emails\n",
    "df_result = df.groupBy('Email').count().filter('count > 1').select('Email')\n",
    "\n",
    "# Show the result\n",
    "df_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b69f8189-77d5-4821-b8b8-204310db05e7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+-------------+\n|customer_id|purchase_amount|purchase_date|\n+-----------+---------------+-------------+\n|          1|            100|   2023-01-15|\n|          2|            150|   2023-02-20|\n|          1|            200|   2023-03-10|\n|          3|             50|   2023-04-05|\n|          2|            120|   2023-05-15|\n|          1|            300|   2023-06-25|\n+-----------+---------------+-------------+\n\n+-----------+---------------------+\n|customer_id|total_purchase_amount|\n+-----------+---------------------+\n|          1|                  600|\n|          2|                  270|\n|          3|                   50|\n+-----------+---------------------+\n\n"
     ]
    }
   ],
   "source": [
    "##Calculate the total purchase amount for each customer:\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum\n",
    "spark = SparkSession.builder.appName(\"CustomerPurchaseAnalysis\").getOrCreate()\n",
    "data = [\n",
    "    (1, 100, \"2023-01-15\"),\n",
    "    (2, 150, \"2023-02-20\"),\n",
    "    (1, 200, \"2023-03-10\"),\n",
    "    (3, 50, \"2023-04-05\"),\n",
    "    (2, 120, \"2023-05-15\"),\n",
    "    (1, 300, \"2023-06-25\")\n",
    "]\n",
    "columns = [\"customer_id\", \"purchase_amount\", \"purchase_date\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "total_purchase_per_customer = df.groupBy('customer_id').agg(sum('purchase_amount').alias('total_purchase_amount'))\n",
    "total_purchase_per_customer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b2de1f5-a0d4-4d12-84ff-635147342136",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+-------------+\n|customer_id|purchase_amount|purchase_date|\n+-----------+---------------+-------------+\n|          1|            100|   2023-01-15|\n|          2|            150|   2023-02-20|\n|          1|            200|   2023-03-10|\n|          3|             50|   2023-04-05|\n|          2|            120|   2023-05-15|\n|          1|            300|   2023-06-25|\n+-----------+---------------+-------------+\n\nOut[4]: 1"
     ]
    }
   ],
   "source": [
    "#Find the customer with the highest total purchase amount\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum, col, desc\n",
    "spark = SparkSession.builder.appName(\"CustomerPurchaseAnalysis\").getOrCreate()\n",
    "data = [\n",
    "    (1, 100, \"2023-01-15\"),\n",
    "    (2, 150, \"2023-02-20\"),\n",
    "    (1, 200, \"2023-03-10\"),\n",
    "    (3, 50, \"2023-04-05\"),\n",
    "    (2, 120, \"2023-05-15\"),\n",
    "    (1, 300, \"2023-06-25\")\n",
    "]\n",
    "columns = [\"customer_id\", \"purchase_amount\", \"purchase_date\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "total_purchase_per_customer = df.groupBy('customer_id').agg(sum('purchase_amount').alias('total_purchase_amount'))\n",
    "customer_with_highest_purchase = total_purchase_per_customer.orderBy(desc('total_purchase_amount')).first()\n",
    "#the above produces row not dataset so cant use show\n",
    "customer_with_highest_purchase['customer_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fa83b13-a82a-4070-8d7a-a9047e0d9573",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----------+-----+-------------+\n|product_id|product_name|   category|price|quantity_sold|\n+----------+------------+-----------+-----+-------------+\n|         1|   Product A|Electronics|  500|          100|\n|         2|   Product B|   Clothing|   50|          200|\n|         3|   Product C|Electronics|  800|           50|\n|         4|   Product D|     Beauty|   30|          300|\n|         5|   Product E|   Clothing|   75|          150|\n+----------+------------+-----------+-----+-------------+\n\n+-------------------+\n|total_rev_all_sales|\n+-------------------+\n|             120250|\n+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Q1.Calculate the total revenue generated from all sales.     \n",
    "          \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum, col, avg\n",
    "from pyspark.sql.window import Window\n",
    "spark = SparkSession.builder.appName(\"Sales\").getOrCreate()\n",
    "data = [\n",
    "    (1, \"Product A\", \"Electronics\",500,100),\n",
    "    (2, \"Product B\", \"Clothing\",50,200),\n",
    "    (3, \"Product C\", \"Electronics\",800,50),\n",
    "    (4, \"Product D\", \"Beauty\",30,300),\n",
    "    (5, \"Product E\", \"Clothing\",75,150)\n",
    "]\n",
    "columns = [\"product_id\", \"product_name\", \"category\",\"price\",\"quantity_sold\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "df.withColumn('total_revenue',col('price')*col('quantity_sold')).agg(sum('total_revenue').alias('total_rev_all_sales')).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06476e84-975e-4cfe-82f4-fff83c310d4e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----------+-----+-------------+\n|product_id|product_name|   category|price|quantity_sold|\n+----------+------------+-----------+-----+-------------+\n|         4|   Product D|     Beauty|   30|          300|\n|         2|   Product B|   Clothing|   50|          200|\n|         5|   Product E|   Clothing|   75|          150|\n|         1|   Product A|Electronics|  500|          100|\n|         3|   Product C|Electronics|  800|           50|\n+----------+------------+-----------+-----+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "## Q2.Find the top 5 best-selling products based on the quantity sold.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum, col, avg\n",
    "from pyspark.sql.window import Window\n",
    "spark = SparkSession.builder.appName(\"Sales\").getOrCreate()\n",
    "data = [\n",
    "    (1, \"Product A\", \"Electronics\",500,100),\n",
    "    (2, \"Product B\", \"Clothing\",50,200),\n",
    "    (3, \"Product C\", \"Electronics\",800,50),\n",
    "    (4, \"Product D\", \"Beauty\",30,300),\n",
    "    (5, \"Product E\", \"Clothing\",75,150)\n",
    "]\n",
    "columns = [\"product_id\", \"product_name\", \"category\",\"price\",\"quantity_sold\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "# df.show()\n",
    "df.orderBy(col('quantity_sold').desc()).limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a201c236-4abd-46b4-9042-3431f68670bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----------+-----+-------------+\n|product_id|product_name|   category|price|quantity_sold|\n+----------+------------+-----------+-----+-------------+\n|         1|   Product A|Electronics|  500|          100|\n|         2|   Product B|   Clothing|   50|          200|\n|         3|   Product C|Electronics|  800|           50|\n|         4|   Product D|     Beauty|   30|          300|\n|         5|   Product E|   Clothing|   75|          150|\n+----------+------------+-----------+-----+-------------+\n\n+-----------+----------------+\n|   category|Avg_per_category|\n+-----------+----------------+\n|Electronics|           650.0|\n|   Clothing|            62.5|\n|     Beauty|            30.0|\n+-----------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "#Q3.Calculate the average price of products in each category.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum, col, avg\n",
    "from pyspark.sql.window import Window\n",
    "spark = SparkSession.builder.appName(\"Sales\").getOrCreate()\n",
    "data = [\n",
    "    (1, \"Product A\", \"Electronics\",500,100),\n",
    "    (2, \"Product B\", \"Clothing\",50,200),\n",
    "    (3, \"Product C\", \"Electronics\",800,50),\n",
    "    (4, \"Product D\", \"Beauty\",30,300),\n",
    "    (5, \"Product E\", \"Clothing\",75,150)\n",
    "]\n",
    "columns = [\"product_id\", \"product_name\", \"category\",\"price\",\"quantity_sold\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "df.groupBy('category').agg(avg('price').alias('Avg_per_category')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64c95ca0-ce3a-49d9-b9b7-eca49b8eaa9f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----------+-----+-------------+\n|product_id|product_name|   category|price|quantity_sold|\n+----------+------------+-----------+-----+-------------+\n|         1|   Product A|Electronics|  500|          100|\n|         2|   Product B|   Clothing|   50|          200|\n|         3|   Product C|Electronics|  800|           50|\n|         4|   Product D|     Beauty|   30|          300|\n|         5|   Product E|   Clothing|   75|          150|\n+----------+------------+-----------+-----+-------------+\n\nOut[8]: 'Electronics'"
     ]
    }
   ],
   "source": [
    "#Q4.Identify the category with the highest total revenue.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum, col, avg\n",
    "from pyspark.sql.window import Window\n",
    "spark = SparkSession.builder.appName(\"Sales\").getOrCreate()\n",
    "data = [\n",
    "    (1, \"Product A\", \"Electronics\",500,100),\n",
    "    (2, \"Product B\", \"Clothing\",50,200),\n",
    "    (3, \"Product C\", \"Electronics\",800,50),\n",
    "    (4, \"Product D\", \"Beauty\",30,300),\n",
    "    (5, \"Product E\", \"Clothing\",75,150)\n",
    "]\n",
    "columns = [\"product_id\", \"product_name\", \"category\",\"price\",\"quantity_sold\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "category_revenue = df.withColumn('revenue',col('price')*col('quantity_sold')).groupBy('category').agg(sum('revenue').alias('total_revenue'))\n",
    "max_category_revenue = category_revenue.orderBy(desc('total_revenue')).first()\n",
    "max_category_revenue['category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77b83730-7822-4112-92c9-0da82095d851",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+-----------+------+\n|employee_id|  employee_name| department|salary|\n+-----------+---------------+-----------+------+\n|          1|       John Doe|Engineering| 90000|\n|          2|     Jane Smith|  Marketing| 75000|\n|          3|Michael Johnson|Engineering|105000|\n|          4|    Emily Davis|  Marketing| 80000|\n|          5|   Robert Brown|Engineering| 95000|\n|          6|   Linda Wilson|         HR| 60000|\n+-----------+---------------+-----------+------+\n\n+-------------+\n|total_payroll|\n+-------------+\n|       505000|\n+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total payroll cost for the company. Calculate the total number of employees in each department. Sample Dataset:\n",
    "# employee_id,employee_name,department,salary 1,John Doe,Engineering,90000 2,Jane Smith,Marketing,75000 3,Michael Johnson,Engineering, # 105000 4,Emily Davis,Marketing,80000 5,Robert Brown,Engineering,95000 6,Linda Wilson,HR,60000\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, avg, max, count\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"EmployeeAnalysis\").getOrCreate()\n",
    "\n",
    "# Load the dataset\n",
    "data = [\n",
    "    (1, \"John Doe\", \"Engineering\", 90000),\n",
    "    (2, \"Jane Smith\", \"Marketing\", 75000),\n",
    "    (3, \"Michael Johnson\", \"Engineering\", 105000),\n",
    "    (4, \"Emily Davis\", \"Marketing\", 80000),\n",
    "    (5, \"Robert Brown\", \"Engineering\", 95000),\n",
    "    (6, \"Linda Wilson\", \"HR\", 60000)\n",
    "]\n",
    "columns = [\"employee_id\", \"employee_name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "df.select(sum(df.salary).alias('total_payroll')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "129573ef-f865-4be3-be42-6774d5d30021",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+-----------+------+\n|employee_id|  employee_name| department|salary|\n+-----------+---------------+-----------+------+\n|          1|       John Doe|Engineering| 90000|\n|          2|     Jane Smith|  Marketing| 75000|\n|          3|Michael Johnson|Engineering|105000|\n|          4|    Emily Davis|  Marketing| 80000|\n|          5|   Robert Brown|Engineering| 95000|\n|          6|   Linda Wilson|         HR| 60000|\n+-----------+---------------+-----------+------+\n\n+-----------+------------------+\n| department|avg_salary_per_dep|\n+-----------+------------------+\n|Engineering| 96666.66666666667|\n|  Marketing|           77500.0|\n|         HR|           60000.0|\n+-----------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Find the average salary for each department.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, avg, max, count\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"EmployeeAnalysis\").getOrCreate()\n",
    "\n",
    "# Load the dataset\n",
    "data = [\n",
    "    (1, \"John Doe\", \"Engineering\", 90000),\n",
    "    (2, \"Jane Smith\", \"Marketing\", 75000),\n",
    "    (3, \"Michael Johnson\", \"Engineering\", 105000),\n",
    "    (4, \"Emily Davis\", \"Marketing\", 80000),\n",
    "    (5, \"Robert Brown\", \"Engineering\", 95000),\n",
    "    (6, \"Linda Wilson\", \"HR\", 60000)\n",
    "]\n",
    "columns = [\"employee_id\", \"employee_name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "df.groupBy('department').agg(avg('salary').alias('avg_salary_per_dep')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b297af80-dd97-4cdb-9e3d-d973e981f76d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+-----------+------+\n|employee_id|  employee_name| department|salary|\n+-----------+---------------+-----------+------+\n|          1|       John Doe|Engineering| 90000|\n|          2|     Jane Smith|  Marketing| 75000|\n|          3|Michael Johnson|Engineering|105000|\n|          4|    Emily Davis|  Marketing| 80000|\n|          5|   Robert Brown|Engineering| 95000|\n|          6|   Linda Wilson|         HR| 60000|\n+-----------+---------------+-----------+------+\n\n+-----------+---------------+-----------+------+-------+\n|employee_id|  employee_name| department|salary|row_num|\n+-----------+---------------+-----------+------+-------+\n|          3|Michael Johnson|Engineering|105000|      1|\n+-----------+---------------+-----------+------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "#Identify the highest-paid employee and their department.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import max, sum, avg, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"EmployeeAnalysis\").getOrCreate()\n",
    "\n",
    "# Load the dataset\n",
    "data = [\n",
    "    (1, \"John Doe\", \"Engineering\", 90000),\n",
    "    (2, \"Jane Smith\", \"Marketing\", 75000),\n",
    "    (3, \"Michael Johnson\", \"Engineering\", 105000),\n",
    "    (4, \"Emily Davis\", \"Marketing\", 80000),\n",
    "    (5, \"Robert Brown\", \"Engineering\", 95000),\n",
    "    (6, \"Linda Wilson\", \"HR\", 60000)\n",
    "]\n",
    "columns = [\"employee_id\", \"employee_name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "w = Window().orderBy(col('salary').desc())\n",
    "df = df.withColumn('row_num', row_number().over(w))\n",
    "# df.show()\n",
    "df.filter('row_num == 1').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "864a9bd2-b73e-4c84-9faf-646784aae96f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+------------+\n|order_id|customer_id|order_date|total_amount|\n+--------+-----------+----------+------------+\n|       1|       C101|2023-07-01|         150|\n|       2|       C102|2023-07-02|         200|\n|       3|       C101|2023-07-02|         100|\n|       4|       C103|2023-07-03|         300|\n|       5|       C102|2023-07-04|         250|\n|       6|       C101|2023-07-05|         120|\n+--------+-----------+----------+------------+\n\n+-------------+\n|total_revenue|\n+-------------+\n|         1120|\n+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "# PySpark Coding Challenge: Analyzing Online Store Orders\n",
    "\n",
    "# Task: You have a dataset containing information about orders from an online store. Your task is to use PySpark to analyze the data and answer a few questions using aggregate functions.\n",
    "\n",
    "# Dataset: The dataset is in CSV format and contains the following columns: order_id, customer_id, order_date, total_amount.\n",
    "\n",
    "# Questions:\n",
    "\n",
    "# Calculate the total revenue generated from all orders.\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, avg, max, count\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"OnlineStoreAnalysis\").getOrCreate()\n",
    "\n",
    "# Load the dataset\n",
    "data = [\n",
    "    (1, \"C101\", \"2023-07-01\", 150),\n",
    "    (2, \"C102\", \"2023-07-02\", 200),\n",
    "    (3, \"C101\", \"2023-07-02\", 100),\n",
    "    (4, \"C103\", \"2023-07-03\", 300),\n",
    "    (5, \"C102\", \"2023-07-04\", 250),\n",
    "    (6, \"C101\", \"2023-07-05\", 120)\n",
    "]\n",
    "columns = [\"order_id\", \"customer_id\", \"order_date\", \"total_amount\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "df.select(sum('total_amount').alias('total_revenue')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f38f8b8-3a0b-4654-8966-c4d76274151f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+------------+\n|order_id|customer_id|order_date|total_amount|\n+--------+-----------+----------+------------+\n|       1|       C101|2023-07-01|         150|\n|       2|       C102|2023-07-02|         200|\n|       3|       C101|2023-07-02|         100|\n|       4|       C103|2023-07-03|         300|\n|       5|       C102|2023-07-04|         250|\n|       6|       C101|2023-07-05|         120|\n+--------+-----------+----------+------------+\n\n+------------------+\n|     avg_order_amt|\n+------------------+\n|186.66666666666666|\n+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Find the average order amount.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, avg, max, count\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"OnlineStoreAnalysis\").getOrCreate()\n",
    "\n",
    "# Load the dataset\n",
    "data = [\n",
    "    (1, \"C101\", \"2023-07-01\", 150),\n",
    "    (2, \"C102\", \"2023-07-02\", 200),\n",
    "    (3, \"C101\", \"2023-07-02\", 100),\n",
    "    (4, \"C103\", \"2023-07-03\", 300),\n",
    "    (5, \"C102\", \"2023-07-04\", 250),\n",
    "    (6, \"C101\", \"2023-07-05\", 120)\n",
    "]\n",
    "columns = [\"order_id\", \"customer_id\", \"order_date\", \"total_amount\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "df.select(avg('total_amount').alias('avg_order_amt')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ead7735-6dc5-46f3-bfa8-840216e6f7b7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+------------+\n|order_id|customer_id|order_date|total_amount|\n+--------+-----------+----------+------------+\n|       1|       C101|2023-07-01|         150|\n|       2|       C102|2023-07-02|         200|\n|       3|       C101|2023-07-02|         100|\n|       4|       C103|2023-07-03|         300|\n|       5|       C102|2023-07-04|         250|\n|       6|       C101|2023-07-05|         120|\n+--------+-----------+----------+------------+\n\n+--------+-----------+----------+------------+-------+\n|order_id|customer_id|order_date|total_amount|row_num|\n+--------+-----------+----------+------------+-------+\n|       4|       C103|2023-07-03|         300|      1|\n+--------+-----------+----------+------------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "# Identify the highest total order amount and its corresponding customer.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, avg, max, count\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"OnlineStoreAnalysis\").getOrCreate()\n",
    "\n",
    "# Load the dataset\n",
    "data = [\n",
    "    (1, \"C101\", \"2023-07-01\", 150),\n",
    "    (2, \"C102\", \"2023-07-02\", 200),\n",
    "    (3, \"C101\", \"2023-07-02\", 100),\n",
    "    (4, \"C103\", \"2023-07-03\", 300),\n",
    "    (5, \"C102\", \"2023-07-04\", 250),\n",
    "    (6, \"C101\", \"2023-07-05\", 120)\n",
    "]\n",
    "columns = [\"order_id\", \"customer_id\", \"order_date\", \"total_amount\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "w = Window().orderBy(col('total_amount').desc())\n",
    "df = df.withColumn('row_num', row_number().over(w))\n",
    "# df.show()\n",
    "df.filter('row_num == 1').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8741b8b2-e1d4-4723-920a-90e49583f39c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+------------+\n|order_id|customer_id|order_date|total_amount|\n+--------+-----------+----------+------------+\n|       1|       C101|2023-07-01|         150|\n|       2|       C102|2023-07-02|         200|\n|       3|       C101|2023-07-02|         100|\n|       4|       C103|2023-07-03|         300|\n|       5|       C102|2023-07-04|         250|\n|       6|       C101|2023-07-05|         120|\n+--------+-----------+----------+------------+\n\n+-----------+---------------------+\n|customer_id|total_orders_per_cust|\n+-----------+---------------------+\n|       C101|                    3|\n|       C102|                    2|\n|       C103|                    1|\n+-----------+---------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total number of orders for each customer.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, avg, max, count\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"OnlineStoreAnalysis\").getOrCreate()\n",
    "\n",
    "# Load the dataset\n",
    "data = [\n",
    "    (1, \"C101\", \"2023-07-01\", 150),\n",
    "    (2, \"C102\", \"2023-07-02\", 200),\n",
    "    (3, \"C101\", \"2023-07-02\", 100),\n",
    "    (4, \"C103\", \"2023-07-03\", 300),\n",
    "    (5, \"C102\", \"2023-07-04\", 250),\n",
    "    (6, \"C101\", \"2023-07-05\", 120)\n",
    "]\n",
    "columns = [\"order_id\", \"customer_id\", \"order_date\", \"total_amount\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "df.groupBy('customer_id').agg(count('order_id').alias('total_orders_per_cust')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87aab78c-99fa-4d10-8ed9-3b5dc950db48",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-----+\n|student_id|subject|score|\n+----------+-------+-----+\n|         1|  Maths|   85|\n|         2|Science|   92|\n|         3|  Maths|   78|\n|         4|English|   88|\n|         5|Science|   95|\n|         6|  Maths|   90|\n+----------+-------+-----+\n\n+-------+-----------------+\n|subject|  avg_per_subject|\n+-------+-----------------+\n|  Maths|84.33333333333333|\n|Science|             93.5|\n|English|             88.0|\n+-------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# DAY 6: 💡Task: You have a dataset containing student exam scores. Your task is to use PySpark to analyze the data and answer a few questions using aggregate functions.\n",
    "\n",
    "# 💡Dataset: The dataset is in CSV format and contains the following columns: student_id, subject, score.\n",
    "\n",
    "# Sample Dataset :\n",
    "\n",
    "# student_id,subject,score 1,Math,85 2,Science,92 3,Math,78 4,English,88 5,Science,95 6,Math,90\n",
    "\n",
    "# 💡𝐂𝐚𝐥𝐜𝐮𝐥𝐚𝐭𝐞 𝐭𝐡𝐞 𝐚𝐯𝐞𝐫𝐚𝐠𝐞 𝐬𝐜𝐨𝐫𝐞 𝐟𝐨𝐫 𝐞𝐚𝐜𝐡 𝐬𝐮𝐛𝐣𝐞𝐜𝐭.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, avg, max, count\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"StudentExamScores\").getOrCreate()\n",
    "\n",
    "# Load the dataset\n",
    "data = [\n",
    "    (1, \"Maths\", 85),\n",
    "    (2, \"Science\", 92),\n",
    "    (3, \"Maths\", 78),\n",
    "    (4, \"English\", 88),\n",
    "    (5, \"Science\", 95),\n",
    "    (6, \"Maths\", 90)\n",
    "]\n",
    "columns = [\"student_id\", \"subject\", \"score\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "df.groupBy('subject').agg(avg('score').alias('avg_per_subject')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "234f2841-6ec7-4b12-8d1c-0f4c63b37185",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-----+\n|student_id|subject|score|\n+----------+-------+-----+\n|         1|  Maths|   85|\n|         2|Science|   92|\n|         3|  Maths|   78|\n|         4|English|   88|\n|         5|Science|   95|\n|         6|  Maths|   90|\n+----------+-------+-----+\n\n+-------+----------+-----+-------------+\n|subject|student_id|score|highest_score|\n+-------+----------+-----+-------------+\n|English|         4|   88|           88|\n|Science|         5|   95|           95|\n|  Maths|         6|   90|           90|\n+-------+----------+-----+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "# 𝐈𝐝𝐞𝐧𝐭𝐢𝐟𝐲 𝐭𝐡𝐞 𝐡𝐢𝐠𝐡𝐞𝐬𝐭 𝐬𝐜𝐨𝐫𝐞 𝐚𝐧𝐝 𝐢𝐭𝐬 𝐜𝐨𝐫𝐫𝐞𝐬𝐩𝐨𝐧𝐝𝐢𝐧𝐠 𝐬𝐭𝐮𝐝𝐞𝐧𝐭 𝐟𝐨𝐫 𝐞𝐚𝐜𝐡 𝐬𝐮𝐛𝐣𝐞𝐜𝐭.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"StudentExamScores\").getOrCreate()\n",
    "\n",
    "# Load the dataset\n",
    "data = [\n",
    "    (1, \"Maths\", 85),\n",
    "    (2, \"Science\", 92),\n",
    "    (3, \"Maths\", 78),\n",
    "    (4, \"English\", 88),\n",
    "    (5, \"Science\", 95),\n",
    "    (6, \"Maths\", 90)\n",
    "]\n",
    "columns = [\"student_id\", \"subject\", \"score\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "highest_score_per_subject = df.groupBy(\"subject\").agg(max(\"score\").alias(\"highest_score\"))\n",
    "# highest_score_per_subject.show()\n",
    "highest_score_students = df.join(highest_score_per_subject, on=\"subject\").filter(col(\"score\") == col(\"highest_score\"))\n",
    "highest_score_students.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c2de1d2-57df-4810-98fb-e3ec71ca06f8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-----+\n|student_id|subject|score|\n+----------+-------+-----+\n|         1|  Maths|   85|\n|         2|Science|   92|\n|         3|  Maths|   78|\n|         4|English|   88|\n|         5|Science|   95|\n|         6|  Maths|   90|\n+----------+-------+-----+\n\n+-------+----------------------+\n|subject|total_students_per_sub|\n+-------+----------------------+\n|  Maths|                     3|\n|Science|                     2|\n|English|                     1|\n+-------+----------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# 𝐂𝐚𝐥𝐜𝐮𝐥𝐚𝐭𝐞 𝐭𝐡𝐞 𝐭𝐨𝐭𝐚𝐥 𝐧𝐮𝐦𝐛𝐞𝐫 𝐨𝐟 𝐬𝐭𝐮𝐝𝐞𝐧𝐭𝐬 𝐰𝐡𝐨 𝐭𝐨𝐨𝐤 𝐞𝐚𝐜𝐡 𝐬𝐮𝐛𝐣𝐞𝐜𝐭.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, avg, max, count\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"StudentExamScores\").getOrCreate()\n",
    "\n",
    "# Load the dataset\n",
    "data = [\n",
    "    (1, \"Maths\", 85),\n",
    "    (2, \"Science\", 92),\n",
    "    (3, \"Maths\", 78),\n",
    "    (4, \"English\", 88),\n",
    "    (5, \"Science\", 95),\n",
    "    (6, \"Maths\", 90)\n",
    "]\n",
    "columns = [\"student_id\", \"subject\", \"score\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "df.groupBy('subject').agg(count('student_id').alias('total_students_per_sub')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12467514-44bc-478a-8a9c-6435cbe3b20c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-----+\n|student_id|subject|score|\n+----------+-------+-----+\n|         1|  Maths|   85|\n|         2|Science|   92|\n|         3|  Maths|   78|\n|         4|English|   88|\n|         5|Science|   95|\n|         6|  Maths|   90|\n+----------+-------+-----+\n\nOut[19]: 'Science'"
     ]
    }
   ],
   "source": [
    "# 💡𝐅𝐢𝐧𝐝 𝐭𝐡𝐞 𝐬𝐮𝐛𝐣𝐞𝐜𝐭(𝐬) 𝐰𝐢𝐭𝐡 𝐭𝐡𝐞 𝐡𝐢𝐠𝐡𝐞𝐬𝐭 𝐚𝐯𝐞𝐫𝐚𝐠𝐞 𝐬𝐜𝐨𝐫𝐞.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, avg, max, count\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"StudentExamScores\").getOrCreate()\n",
    "\n",
    "# Load the dataset\n",
    "data = [\n",
    "    (1, \"Maths\", 85),\n",
    "    (2, \"Science\", 92),\n",
    "    (3, \"Maths\", 78),\n",
    "    (4, \"English\", 88),\n",
    "    (5, \"Science\", 95),\n",
    "    (6, \"Maths\", 90)\n",
    "]\n",
    "columns = [\"student_id\", \"subject\", \"score\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "df = df.groupBy('subject').agg(avg('score').alias('avg_per_subject'))\n",
    "max_avg = df.orderBy(col('avg_per_subject').desc()).first()\n",
    "max_avg['subject']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c65873ba-1101-4afc-8edd-8629b9fd9da4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n|user_id|          timestamp|\n+-------+-------------------+\n|  user1|2023-08-21 10:00:00|\n|  user2|2023-08-21 11:30:00|\n|  user1|2023-08-21 12:15:00|\n|  user3|2023-08-21 13:45:00|\n|  user2|2023-08-21 14:30:00|\n|  user1|2023-08-21 15:00:00|\n+-------+-------------------+\n\n+-------------------+\n| earliest_timestamp|\n+-------------------+\n|2023-08-21 10:00:00|\n+-------------------+\n\n+-------------------+\n|   latest_timestamp|\n+-------------------+\n|2023-08-21 15:00:00|\n+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Questions:1 Sample Dataset :\n",
    "\n",
    "# columns = [\"user_id\", \"timestamp\"]\n",
    "\n",
    "# data= [(\"user1\", \"2023-08-21 10:00:00\"), (\"user2\", \"2023-08-21 11:30:00\"), (\"user1\", \"2023-08-21 12:15:00\"), (\"user3\", \"2023-08-21 13:45:00\"), (\"user2\", \"2023-08-21 14:30:00\"), (\"user1\", \"2023-08-21 15:00:00\")]\n",
    "\n",
    "# 💡 𝐅𝐢𝐧𝐝 𝐭𝐡𝐞 𝐞𝐚𝐫𝐥𝐢𝐞𝐬𝐭 𝐚𝐧𝐝 𝐥𝐚𝐭𝐞𝐬𝐭 𝐭𝐢𝐦𝐞𝐬𝐭𝐚𝐦𝐩𝐬 𝐢𝐧 𝐭𝐡𝐞 𝐝𝐚𝐭𝐚𝐬𝐞𝐭.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, to_timestamp, min, max\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"StudentExamScores\").getOrCreate()\n",
    "\n",
    "# Load the dataset\n",
    "data= [\n",
    "    (\"user1\", \"2023-08-21 10:00:00\"),\n",
    "    (\"user2\", \"2023-08-21 11:30:00\"),\n",
    "    (\"user1\", \"2023-08-21 12:15:00\"),\n",
    "    (\"user3\", \"2023-08-21 13:45:00\"),\n",
    "    (\"user2\", \"2023-08-21 14:30:00\"),\n",
    "    (\"user1\", \"2023-08-21 15:00:00\")\n",
    "    ]\n",
    "columns = [\"user_id\", \"timestamp\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df = df.withColumn('timestamp', to_timestamp(col('timestamp')))\n",
    "df.show()\n",
    "earliest_timestamp = df.select(min('timestamp').alias('earliest_timestamp'))\n",
    "earliest_timestamp.show()\n",
    "latest_timestamp = df.select(max('timestamp').alias('latest_timestamp'))\n",
    "latest_timestamp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb62c746-1fdc-4b21-ade4-c7cf29ad7a67",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n|user_id|          timestamp|\n+-------+-------------------+\n|  user1|2023-08-21 10:00:00|\n|  user2|2023-08-21 11:30:00|\n|  user1|2023-08-21 12:15:00|\n|  user3|2023-08-21 13:45:00|\n|  user2|2023-08-21 14:30:00|\n|  user1|2023-08-21 15:00:00|\n+-------+-------------------+\n\n+-------+----------------+\n|user_id|no_of_activities|\n+-------+----------------+\n|  user1|               3|\n|  user2|               2|\n|  user3|               1|\n+-------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# 💡 𝐂𝐨𝐮𝐧𝐭 𝐭𝐡𝐞 𝐧𝐮𝐦𝐛𝐞𝐫 𝐨𝐟 𝐚𝐜𝐭𝐢𝐯𝐢𝐭𝐢𝐞𝐬 𝐩𝐞𝐫 𝐮𝐬𝐞𝐫.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, avg, max, count\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"StudentExamScores\").getOrCreate()\n",
    "\n",
    "# Load the dataset\n",
    "data= [\n",
    "    (\"user1\", \"2023-08-21 10:00:00\"),\n",
    "    (\"user2\", \"2023-08-21 11:30:00\"),\n",
    "    (\"user1\", \"2023-08-21 12:15:00\"),\n",
    "    (\"user3\", \"2023-08-21 13:45:00\"),\n",
    "    (\"user2\", \"2023-08-21 14:30:00\"),\n",
    "    (\"user1\", \"2023-08-21 15:00:00\")\n",
    "    ]\n",
    "columns = [\"user_id\", \"timestamp\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df = df.withColumn('timestamp', to_timestamp(col('timestamp')))\n",
    "df.show()\n",
    "df.groupBy('user_id').agg(count('timestamp').alias('no_of_activities')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77601ec5-8cc2-44ba-b197-17d5e7a18e6c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n|user_id|          timestamp|\n+-------+-------------------+\n|  user1|2023-08-21 10:00:00|\n|  user2|2023-08-21 11:30:00|\n|  user1|2023-08-21 12:15:00|\n|  user3|2023-08-21 13:45:00|\n|  user2|2023-08-21 14:30:00|\n|  user1|2023-08-21 15:00:00|\n+-------+-------------------+\n\n+-------+-------------------+-------------------+\n|user_id|          timestamp|     prev_timestamp|\n+-------+-------------------+-------------------+\n|  user1|2023-08-21 10:00:00|               null|\n|  user1|2023-08-21 12:15:00|2023-08-21 10:00:00|\n|  user1|2023-08-21 15:00:00|2023-08-21 12:15:00|\n|  user2|2023-08-21 11:30:00|               null|\n|  user2|2023-08-21 14:30:00|2023-08-21 11:30:00|\n|  user3|2023-08-21 13:45:00|               null|\n+-------+-------------------+-------------------+\n\n+-------+-------------------+-------------------+-------------+\n|user_id|          timestamp|     prev_timestamp|time_duration|\n+-------+-------------------+-------------------+-------------+\n|  user1|2023-08-21 10:00:00|               null|         null|\n|  user1|2023-08-21 12:15:00|2023-08-21 10:00:00|         8100|\n|  user1|2023-08-21 15:00:00|2023-08-21 12:15:00|         9900|\n|  user2|2023-08-21 11:30:00|               null|         null|\n|  user2|2023-08-21 14:30:00|2023-08-21 11:30:00|        10800|\n|  user3|2023-08-21 13:45:00|               null|         null|\n+-------+-------------------+-------------------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "# 💡𝐂𝐚𝐥𝐜𝐮𝐥𝐚𝐭𝐞 𝐭𝐡𝐞 𝐭𝐢𝐦𝐞 𝐝𝐮𝐫𝐚𝐭𝐢𝐨𝐧 𝐛𝐞𝐭𝐰𝐞𝐞𝐧 𝐜𝐨𝐧𝐬𝐞𝐜𝐮𝐭𝐢𝐯𝐞 𝐚𝐜𝐭𝐢𝐯𝐢𝐭𝐢𝐞𝐬 𝐟𝐨𝐫 𝐞𝐚𝐜𝐡 𝐮𝐬𝐞𝐫.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, avg, max, count, lag, cast\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"StudentExamScores\").getOrCreate()\n",
    "\n",
    "# Load the dataset\n",
    "data= [\n",
    "    (\"user1\", \"2023-08-21 10:00:00\"),\n",
    "    (\"user2\", \"2023-08-21 11:30:00\"),\n",
    "    (\"user1\", \"2023-08-21 12:15:00\"),\n",
    "    (\"user3\", \"2023-08-21 13:45:00\"),\n",
    "    (\"user2\", \"2023-08-21 14:30:00\"),\n",
    "    (\"user1\", \"2023-08-21 15:00:00\")\n",
    "    ]\n",
    "columns = [\"user_id\", \"timestamp\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df = df.withColumn('timestamp', to_timestamp(col('timestamp')))\n",
    "df.show()\n",
    "# print(df.dtypes)\n",
    "# print(df.schema)\n",
    "w = Window().partitionBy('user_id').orderBy('timestamp')\n",
    "df_with_prev_timestamp = df.withColumn('prev_timestamp',lag('timestamp').over(w))\n",
    "df_with_prev_timestamp.show()\n",
    "df_with_prev_timestamp.withColumn('time_duration',col('timestamp').cast('long')- col('prev_timestamp').cast('long')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1759baf9-a2fd-4ea9-8146-75c8c41e95c9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------------------+\n|user_id|action  |timestamp          |\n+-------+--------+-------------------+\n|1      |login   |2023-08-20 10:23:45|\n|2      |view    |2023-08-20 11:15:30|\n|1      |purchase|2023-08-20 12:45:18|\n|3      |view    |2023-08-20 13:30:22|\n+-------+--------+-------------------+\n\n+-------------+\n|count(action)|\n+-------------+\n|            4|\n+-------------+\n\n+--------+\n|  action|\n+--------+\n|   login|\n|    view|\n|purchase|\n+--------+\n\n+-------+--------+-------------------+-------------------+\n|user_id|  action|          timestamp|     prev_timestamp|\n+-------+--------+-------------------+-------------------+\n|      1|   login|2023-08-20 10:23:45|               null|\n|      1|purchase|2023-08-20 12:45:18|2023-08-20 10:23:45|\n|      2|    view|2023-08-20 11:15:30|               null|\n|      3|    view|2023-08-20 13:30:22|               null|\n+-------+--------+-------------------+-------------------+\n\n+-------+--------+-------------------+-------------------+-------------+\n|user_id|  action|          timestamp|     prev_timestamp|time_duration|\n+-------+--------+-------------------+-------------------+-------------+\n|      1|   login|2023-08-20 10:23:45|               null|         null|\n|      1|purchase|2023-08-20 12:45:18|2023-08-20 10:23:45|         8493|\n|      2|    view|2023-08-20 11:15:30|               null|         null|\n|      3|    view|2023-08-20 13:30:22|               null|         null|\n+-------+--------+-------------------+-------------------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Questions:2\n",
    "\n",
    "# Count the number of action\n",
    "# 2: What are the unique actions recorded in the dataset?\n",
    "# 3: Calculate the time duration between consecutive activities for each user\n",
    "# data = [ (1, \"login\", \"2023-08-20 10:23:45\"), (2, \"view\", \"2023-08-20 11:15:30\"), (1, \"purchase\", \"2023-08-20 12:45:18\"), (3, \"view\", \"2023-08-20 13:30:22\") ] columns = [\"user_id\", \"action\", \"timestamp\"]\n",
    "\n",
    "#import relevant libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType,FloatType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank,col,min,max,row_number,max,asc,desc,when,count,sum\n",
    "#Let's create Sparksession First\n",
    "\n",
    "\n",
    "spark=SparkSession.builder.master(\"local[1]\").appName(\"Actions\").getOrCreate()\n",
    "\n",
    "data = [\n",
    "    (1, \"login\", \"2023-08-20 10:23:45\"),\n",
    "    (2, \"view\", \"2023-08-20 11:15:30\"),\n",
    "    (1, \"purchase\", \"2023-08-20 12:45:18\"),\n",
    "    (3, \"view\", \"2023-08-20 13:30:22\")\n",
    "]\n",
    "columns = [\"user_id\", \"action\", \"timestamp\"]\n",
    "#Schema\n",
    "\n",
    "columns = StructType([StructField(\"user_id\",StringType()), StructField(\"action\",StringType()),StructField(\"timestamp\", StringType())])\n",
    "action_df = spark.createDataFrame(data, columns)\n",
    "action_df = action_df.withColumn('timestamp', to_timestamp(col('timestamp')))\n",
    "action_df.show(truncate=False)\n",
    "\n",
    "#1\n",
    "action_df.select(count('action')).show()\n",
    "#2\n",
    "action_df.select(col('action')).distinct().show()\n",
    "#3\n",
    "w = Window().partitionBy('user_id').orderBy('timestamp')\n",
    "df_with_prev_timestamp = action_df.withColumn('prev_timestamp',lag('timestamp').over(w))\n",
    "df_with_prev_timestamp.show()\n",
    "df_with_prev_timestamp.withColumn('time_duration',col('timestamp').cast('long')- col('prev_timestamp').cast('long')).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22f15879-de82-437c-ace2-2347d41a4ed1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+------+\n|name  |dept_name|Salary|\n+------+---------+------+\n|James |Sales    |2000  |\n|sofy  |Sales    |3000  |\n|Laren |Sales    |4000  |\n|Kiku  |Sales    |5000  |\n|Sam   |Finance  |6000  |\n|Samuel|Finance  |7000  |\n|Yash  |Finance  |8000  |\n|Rabin |Finance  |9000  |\n|Lukasz|Marketing|10000 |\n|Jolly |Marketing|11000 |\n|Mausam|Marketing|12000 |\n|Lamba |Marketing|13000 |\n|Jogesh|HR       |14000 |\n|Mannu |HR       |15000 |\n|Sylvia|HR       |16000 |\n|Sama  |HR       |17000 |\n+------+---------+------+\n\n+------+---------+------+---------+\n|  name|dept_name|Salary|cumu_dist|\n+------+---------+------+---------+\n|   Sam|  Finance|  6000|     0.25|\n|Samuel|  Finance|  7000|      0.5|\n|  Yash|  Finance|  8000|     0.75|\n| Rabin|  Finance|  9000|      1.0|\n|Jogesh|       HR| 14000|     0.25|\n| Mannu|       HR| 15000|      0.5|\n|Sylvia|       HR| 16000|     0.75|\n|  Sama|       HR| 17000|      1.0|\n|Lukasz|Marketing| 10000|     0.25|\n| Jolly|Marketing| 11000|      0.5|\n|Mausam|Marketing| 12000|     0.75|\n| Lamba|Marketing| 13000|      1.0|\n| James|    Sales|  2000|     0.25|\n|  sofy|    Sales|  3000|      0.5|\n| Laren|    Sales|  4000|     0.75|\n|  Kiku|    Sales|  5000|      1.0|\n+------+---------+------+---------+\n\n+---------+----------+----------+\n|dept_name|min_salary|max_salary|\n+---------+----------+----------+\n|Finance  |6000      |9000      |\n|HR       |14000     |17000     |\n|Marketing|10000     |13000     |\n|Sales    |2000      |5000      |\n+---------+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Q2 . Find out min, max and cumulative salary in a dataset with emp_name, dept_name and salary as columns\n",
    "\n",
    "# some interviewer will specifically ask you to Create a sparksession, dataframe , define schema and then write the logic.\n",
    "\n",
    "# (Asked in Service based companies)\n",
    "\n",
    "# Best Answer \n",
    "\n",
    "#import relevant libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType,FloatType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank,col,min,max,row_number,max,asc,desc,when,count,sum, cume_dist\n",
    "#Let's create Sparksession First\n",
    "\n",
    "\n",
    "spark=SparkSession.builder.master(\"local[1]\").appName(\"Prep\").getOrCreate()\n",
    "\n",
    "data= [(\"James\", \"Sales\", 2000),\n",
    "(\"sofy\", \"Sales\", 3000),\n",
    "(\"Laren\", \"Sales\", 4000),\n",
    "(\"Kiku\", \"Sales\", 5000),\n",
    "(\"Sam\", \"Finance\", 6000),\n",
    "(\"Samuel\", \"Finance\", 7000),\n",
    "(\"Yash\", \"Finance\", 8000),\n",
    "(\"Rabin\", \"Finance\", 9000),\n",
    "(\"Lukasz\", \"Marketing\", 10000),\n",
    "(\"Jolly\", \"Marketing\", 11000),\n",
    "(\"Mausam\", \"Marketing\", 12000),\n",
    "(\"Lamba\", \"Marketing\", 13000),\n",
    "(\"Jogesh\", \"HR\", 14000),\n",
    "(\"Mannu\", \"HR\", 15000),\n",
    "(\"Sylvia\", \"HR\", 16000),\n",
    "(\"Sama\", \"HR\", 17000),\n",
    "]\n",
    "\n",
    "#Schema\n",
    "\n",
    "emp_schema = StructType([StructField(\"name\",StringType()), StructField(\"dept_name\",StringType()),StructField(\"Salary\", StringType())])\n",
    "employees_Salary_df = spark.createDataFrame(data, emp_schema)\n",
    "employees_Salary_df.show(truncate=False)\n",
    "\n",
    "#creating window\n",
    "w = Window().partitionBy('dept_name').orderBy('Salary')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#getting cumulative salary\n",
    "employees_Salary_df.withColumn('cumu_dist',cume_dist().over(w)).show()\n",
    "\n",
    "\n",
    "#getting min and max salary\n",
    "employees_Salary_df.groupBy('dept_name').agg(min('Salary').alias('min_salary'),max('Salary').alias('max_salary')).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8793953a-f732-40c3-8af0-974287232543",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "🌟 Pyspark scenario questions : 🤷‍♂️\n",
    "==================\n",
    "👉 Here in this question we used LAG(), datediff() and Aggregate MAX() functions.\n",
    "\n",
    "✔ LAG() function is a window function that returns the value that is offset rows before the current row, and defaults if there are less than offset rows before the current row. This is equivalent to the LAG function in SQL. The PySpark Window functions operate on a group of rows (like frame, partition) and return a single value for every input row.\n",
    "\n",
    "Imagine a vast customer interaction dataset, and you want to understand the maximum time gap between consecutive interactions for each customer. This intricate task can be efficiently accomplished using PySpark's LAG function in conjunction with the Aggregate MAX function.\n",
    "\n",
    "🔅 Challenge:\n",
    "------------\n",
    "Calculate the longest time duration between two consecutive customer interactions using the PySpark LAG function. Moreover, you want to find the maximum of these time gaps using the Aggregate MAX function. This intricate analysis can offer valuable insights into customer engagement patterns.\n",
    "\n",
    "🔑 Solution :\n",
    "----------\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ComplexFunctionExample\").getOrCreate()\n",
    "\n",
    "hashtag#sample  customer interaction data\n",
    "data = [(\"Cust1\", \"2023-07-01\", \"Interaction\"),\n",
    " (\"Cust1\", \"2023-07-03\", \"Interaction\"),\n",
    " (\"Cust1\", \"2023-07-05\", \"Interaction\"),\n",
    " (\"Cust2\", \"2023-07-02\", \"Interaction\"),\n",
    " (\"Cust2\", \"2023-07-06\", \"Interaction\")]\n",
    "\n",
    "hashtag#create  Dataframe\n",
    "columns = [\"customer_id\",\"interaction_date\",\"interaction_type\"]\n",
    "df = spark.createDataFrame(data,columns)\n",
    "\n",
    "\n",
    "output:\n",
    "=====\n",
    "+-----------+------------+\n",
    "|customer_id|max_time_gap|\n",
    "+-----------+------------+\n",
    "|   Cust2|      4|\n",
    "|   Cust1|      2|\n",
    "+-----------+------------+\n",
    "\n",
    "📈 Logic :- we used LAG() function to compare the interaction of previous customer at specified window, used datediff() to get the difference between interactions, Aggregation MAX() to pull out the maximum time gap.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f84e4fb0-b27f-451e-a617-7dc6192c95d7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+----------------+\n|customer_id|interaction_date|interaction_type|\n+-----------+----------------+----------------+\n|Cust1      |2023-07-01      |Interaction     |\n|Cust1      |2023-07-03      |Interaction     |\n|Cust1      |2023-07-05      |Interaction     |\n|Cust2      |2023-07-02      |Interaction     |\n|Cust2      |2023-07-06      |Interaction     |\n+-----------+----------------+----------------+\n\n+-----------+----------------+\n|customer_id|maximum time gap|\n+-----------+----------------+\n|      Cust1|               2|\n|      Cust2|               4|\n+-----------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType,FloatType\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName(\"ComplexFunctionExample\").getOrCreate()\n",
    "\n",
    "#sample  customer interaction data\n",
    "data = [(\"Cust1\", \"2023-07-01\", \"Interaction\"),\n",
    " (\"Cust1\", \"2023-07-03\", \"Interaction\"),\n",
    " (\"Cust1\", \"2023-07-05\", \"Interaction\"),\n",
    " (\"Cust2\", \"2023-07-02\", \"Interaction\"),\n",
    " (\"Cust2\", \"2023-07-06\", \"Interaction\")]\n",
    "\n",
    "#create  Dataframe\n",
    "\n",
    "schema = StructType([StructField(\"customer_id\",StringType()), StructField(\"interaction_date\",StringType()),StructField(\"interaction_type\", StringType())])\n",
    "employees_Salary_df = spark.createDataFrame(data, schema)\n",
    "employees_Salary_df.show(truncate=False)\n",
    "\n",
    "w = Window().partitionBy('customer_id').orderBy('interaction_date')\n",
    "df_prev_date = employees_Salary_df.withColumn('prev_date',F.lag('interaction_date').over(w))\n",
    "df_date_diff = df_prev_date.withColumn('diff_date',F.datediff(col('interaction_date'),col('prev_date')))\n",
    "df_date_diff.groupBy('customer_id').agg(max('diff_date').alias('maximum time gap')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "697a565f-66a6-4cf0-9361-6765274c6db9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+------------+------------+-----------+-----------+\n|sender_id|send_to_id|request_date|requester_id|accepter_id|accept_date|\n+---------+----------+------------+------------+-----------+-----------+\n|        1|         2|  2016/06/01|           1|          2| 2016/06/03|\n|        1|         3|  2016/06/01|           1|          3| 2016/06/08|\n|        1|         3|  2016/06/01|           1|          3| 2016/06/08|\n+---------+----------+------------+------------+-----------+-----------+\n\ntotal_requests : 3\nunique_requests : 2\n+-------------+---------------+----------------+\n|total_request|unique_requests|accepatance_rate|\n+-------------+---------------+----------------+\n|            3|              2|            0.67|\n+-------------+---------------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Write a Pyspark query to find the overall acceptance rate of requests, which is the number of acceptance divided by the number of requests. Return the answer rounded to 2 decimals places. If there are duplicate requests consider them only.If there are no requests at all, you should return 0.00 as the accept_rate.\n",
    "\n",
    "# FriendRequest table: +-----------+------------+--------------+ | sender_id | send_to_id | request_date | +-----------+------------+--------------+ | 1 | 2 | 2016/06/01 | | 1 | 3 | 2016/06/01 | | 1 | 3 | 2016/06/01 | +-----------+------------+--------------+\n",
    "\n",
    "# RequestAccepted table: +--------------+-------------+-------------+ | requester_id | accepter_id | accept_date | +--------------+-------------+-------------+ | 1 | 2 | 2016/06/03 | | 1 | 3 | 2016/06/08 | +--------------+-------------+-------------+\n",
    "\n",
    "# Result table: +-------------+ | unique_accepted_request | +-------------+ | 2 | +-------------+\n",
    "\n",
    "# +-------------+ | total_request | +-------------+ | 3 | +-------------+\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, avg, max, count\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"FriendRequest\").getOrCreate()\n",
    "spark1 = SparkSession.builder.appName(\"RequestAccepted\").getOrCreate()\n",
    "\n",
    "# Load the dataset\n",
    "data = [\n",
    "    (1, 2, \"2016/06/01\"),\n",
    "    (1, 3, \"2016/06/01\"),\n",
    "    (1, 3, \"2016/06/01\")\n",
    "]\n",
    "data1 = [\n",
    "    (1, 2, \"2016/06/03\"),\n",
    "    (1, 3, \"2016/06/08\")\n",
    "]\n",
    "columns = [\"sender_id\", \"send_to_id\", \"request_date\"]\n",
    "columns1 = [\"requester_id\", \"accepter_id\", \"accept_date\"]\n",
    "df_FR = spark.createDataFrame(data, columns)\n",
    "df_RA = spark.createDataFrame(data1, columns1)\n",
    "# df_FR.show()\n",
    "# df_RA.show()\n",
    "\n",
    "# Step 01: Join data frames based on conditions\n",
    "joined_df = df_FR.join(df_RA, (df_FR.sender_id == df_RA.requester_id) & (df_FR.send_to_id == df_RA.accepter_id),'inner')\n",
    "joined_df.show()\n",
    "# Step 02: Total requests and unique accepted requests\n",
    "total_request = joined_df.count()\n",
    "print('total_requests :', total_request)\n",
    "unique_requests = joined_df.select('requester_id','accepter_id').distinct().count()\n",
    "print('unique_requests :',unique_requests)\n",
    "# Step 03: Calculate the acceptance rate\n",
    "accepatance_rate = 0 if total_request == 0 else round(unique_requests/total_request,2)\n",
    "# Step 04: Show the result\n",
    "result_df = spark.createDataFrame([(total_request, unique_requests, accepatance_rate)],['total_request', 'unique_requests', 'accepatance_rate'])\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "448f7917-22b4-4b4f-a562-2d861db00104",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Every Advance Function\n",
    "You have a dataset containing user activity logs in a PySpark DataFrame with the following columns: user_id, timestamp, and action. The action column indicates whether the user started or ended a session. It can have values 'start' or 'end'. Your task is to calculate the average duration of user sessions.\n",
    "\n",
    "data = [ (1, \"2022-01-01 10:00\", \"start\"), (1, \"2022-01-01 10:15\", \"end\"), (2, \"2022-01-01 11:00\", \"start\"), (1, \"2022-01-01 11:30\", \"start\"), (2, \"2022-01-01 11:45\", \"end\"), (1, \"2022-01-01 12:00\", \"end\") ] schema = [\"user_id\", \"timestamp\", \"action\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccb6b208-4743-4ba7-a8b7-47379f6e593c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "NTILE() window function returns the relative rank of result rows within a window partition. In below example we have used 2 as an argument to ntile hence it returns ranking between 2 values (1 and 2)\n",
    "\n",
    "RANK() window function is used to provide a rank to the result within a window partition. This function leaves gaps in rank when there are ties.\n",
    "\n",
    "DENSE_RANK() window function is used to get the result with rank of rows within a window partition without any gaps. This is similar to RANK() function difference being rank function leaves gaps in rank when there are ties.\n",
    "\n",
    "ROW_NUMBER() window function is used to give the sequential row number starting from 1 to the result of each window partition.\n",
    "\n",
    "LAG() is a function that works as the offset row returning the value of the before row of a column with respect to the current row.\n",
    "\n",
    "LEAD() is a function that works as the offset row returning the value of the after row of a column with respect to the current row.\n",
    "\n",
    "PERCENTILE_RANK() Returns the percentile rank of rows within a window partition.\n",
    "\n",
    "Here is the Code 👇 :\n",
    "--------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0593ca87-a48b-4442-a4c7-add88275eadf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------+----------+----+----------+------------+----+----+-----+\n|   name|     dept|salary|row_number|rank|dense_rank|percent_rank| lag|lead|ntile|\n+-------+---------+------+----------+----+----------+------------+----+----+-----+\n|  Maria|  Finance|  3000|         1|   1|         1|         0.0|null|3900|    1|\n|  Scott|  Finance|  3300|         2|   2|         2|         0.5|null|null|    1|\n|    Jen|  Finance|  3900|         3|   3|         3|         1.0|3000|null|    2|\n|  Kumar|Marketing|  2000|         1|   1|         1|         0.0|null|null|    1|\n|   Jeff|Marketing|  3000|         2|   2|         2|         1.0|null|null|    2|\n|  James|    Sales|  3000|         1|   1|         1|         0.0|null|4100|    1|\n|  James|    Sales|  3000|         2|   1|         1|         0.0|null|4100|    1|\n| Robert|    Sales|  4100|         3|   3|         2|         0.5|3000|4600|    1|\n|   Saif|    Sales|  4100|         4|   3|         2|         0.5|3000|null|    2|\n|Michael|    Sales|  4600|         5|   5|         3|         1.0|4100|null|    2|\n+-------+---------+------+----------+----+----------+------------+----+----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank, row_number, dense_rank, lag, lead, percent_rank, ntile\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Window_functions\").getOrCreate()\n",
    "\n",
    "simpleData = ((\"James\", \"Sales\", 3000),\n",
    " (\"Michael\", \"Sales\", 4600),\n",
    " (\"Robert\", \"Sales\", 4100),\n",
    " (\"Maria\", \"Finance\", 3000),\n",
    " (\"James\", \"Sales\", 3000),\n",
    " (\"Scott\", \"Finance\", 3300),\n",
    " (\"Jen\", \"Finance\", 3900),\n",
    " (\"Jeff\", \"Marketing\", 3000),\n",
    " (\"Kumar\", \"Marketing\", 2000),\n",
    " (\"Saif\", \"Sales\", 4100)\n",
    " )\n",
    "columns = (\"name\",\"dept\",\"salary\")\n",
    "\n",
    "#creating a dataframe\n",
    "df = spark.createDataFrame(simpleData,columns)\n",
    "\n",
    "#creating a Window specification\n",
    "window_spec = Window.partitionBy(\"dept\").orderBy(\"salary\")\n",
    "\n",
    "#Applying window functions\n",
    "df.withColumn(\"row_number\",row_number().over(window_spec))\\\n",
    " .withColumn(\"rank\",rank().over(window_spec))\\\n",
    " .withColumn(\"dense_rank\",dense_rank().over(window_spec))\\\n",
    " .withColumn(\"percent_rank\",percent_rank().over(window_spec))\\\n",
    " .withColumn(\"lag\",lag(\"salary\",2).over(window_spec))\\\n",
    " .withColumn(\"lead\",lead(\"salary\",2).over(window_spec))\\\n",
    " .withColumn(\"ntile\",ntile(2).over(window_spec))\\\n",
    " .show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Pyspark 1",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
