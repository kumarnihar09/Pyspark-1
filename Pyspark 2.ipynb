{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed034102-0af7-4f2f-8bf6-968da0c5cec9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+\n|userid|timestamp          |\n+------+-------------------+\n|user1 |2023-08-26 10:10:00|\n|user1 |2023-08-26 10:10:25|\n|user2 |2023-08-26 12:00:00|\n|user2 |2023-08-26 12:10:00|\n|user1 |2023-08-26 14:30:00|\n|user1 |2023-08-26 16:00:00|\n|user2 |2023-08-26 16:30:00|\n|user1 |2023-08-26 18:00:00|\n+------+-------------------+\n\n+------+-------------------+-------------------+----------------------+\n|userid|          timestamp|        lag_session|total_time_per_session|\n+------+-------------------+-------------------+----------------------+\n| user1|2023-08-26 10:10:25|2023-08-26 10:10:00|                    25|\n| user1|2023-08-26 14:30:00|2023-08-26 10:10:25|                 15575|\n| user1|2023-08-26 16:00:00|2023-08-26 14:30:00|                  5400|\n| user1|2023-08-26 18:00:00|2023-08-26 16:00:00|                  7200|\n| user2|2023-08-26 12:10:00|2023-08-26 12:00:00|                   600|\n| user2|2023-08-26 16:30:00|2023-08-26 12:10:00|                 15600|\n+------+-------------------+-------------------+----------------------+\n\n+------+--------------------------+\n|userid|total_sessiontime_per_user|\n+------+--------------------------+\n| user1|                     28200|\n| user2|                     16200|\n+------+--------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# 💡 𝐂𝐚𝐥𝐜𝐮𝐥𝐚𝐭𝐞 𝐭𝐨𝐭𝐚𝐥 𝐭𝐢𝐦𝐞 𝐬𝐩𝐞𝐧𝐭 𝐩𝐞𝐫 𝐬𝐞𝐬𝐬𝐢𝐨𝐧?\n",
    "# 💡 𝐂𝐚𝐥𝐜𝐮𝐥𝐚𝐭𝐞 𝐭𝐨𝐭𝐚𝐥 𝐬𝐞𝐬𝐬𝐢𝐨𝐧 𝐭𝐢𝐦𝐞 𝐩𝐞𝐫 𝐮𝐬𝐞𝐫?\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank,col,min,max,row_number,max,lag,count,sum, to_timestamp\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName(\"SessionTime\").getOrCreate()\n",
    "\n",
    "#sample  customer interaction data\n",
    "data = [\n",
    " (\"user1\", \"2023-08-26 10:10:00\"),\n",
    " (\"user1\", \"2023-08-26 10:10:25\"),\n",
    " (\"user2\", \"2023-08-26 12:00:00\"),\n",
    " (\"user2\", \"2023-08-26 12:10:00\"),\n",
    " (\"user1\", \"2023-08-26 14:30:00\"),\n",
    " (\"user1\", \"2023-08-26 16:00:00\"),\n",
    " (\"user2\", \"2023-08-26 16:30:00\"),\n",
    " (\"user1\", \"2023-08-26 18:00:00\"),\n",
    "]\n",
    "schema = StructType([\n",
    " StructField(\"userid\", StringType(), nullable=False), StructField(\"timestamp\", StringType(), nullable=False)])\n",
    "\n",
    "#create  Dataframe\n",
    "\n",
    "sessions_df = spark.createDataFrame(data, schema)\n",
    "sessions_df = sessions_df.withColumn('timestamp', to_timestamp(col('timestamp')))\n",
    "sessions_df.show(truncate=False)\n",
    "w = Window().partitionBy('userid').orderBy('timestamp')\n",
    "sessions_df = sessions_df.withColumn('lag_session',lag('timestamp').over(w))\n",
    "sessions_df = sessions_df.withColumn('total_time_per_session', col('timestamp').cast('long') - col('lag_session').cast('long'))\n",
    "sessions_df = sessions_df.where(col('lag_session').isNotNull())\n",
    "sessions_df.show()\n",
    "sessions_df = sessions_df.groupBy('userid').agg(sum('total_time_per_session').alias('total_sessiontime_per_user')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b86ad2c6-641f-4a5d-bc60-de3673865f1f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n|Id |NameCust|\n+---+--------+\n|1  |Joe     |\n|2  |Henry   |\n|3  |Sam     |\n|4  |Max     |\n+---+--------+\n\n+---+----------+\n|Id |CustomerId|\n+---+----------+\n|1  |3         |\n|2  |1         |\n+---+----------+\n\n+---+--------+----+----------+\n| Id|NameCust|  Id|CustomerId|\n+---+--------+----+----------+\n|  1|     Joe|   2|         1|\n|  2|   Henry|null|      null|\n|  3|     Sam|   1|         3|\n|  4|     Max|null|      null|\n+---+--------+----+----------+\n\n+---+--------+----+----------+\n| Id|NameCust|  Id|CustomerId|\n+---+--------+----+----------+\n|  2|   Henry|null|      null|\n|  4|     Max|null|      null|\n+---+--------+----+----------+\n\n+--------+\n|NameCust|\n+--------+\n|   Henry|\n|     Max|\n+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# Write a pyspark dataframe query to find all customers who never order anything.\n",
    "\n",
    "# Table: Customers.\n",
    "\n",
    "# +----+-------+ | Id | NameCust | +----+-------+ | 1 | Joe | | 2 | Henry | | 3 | Sam | | 4 | Max | +----+-------+\n",
    "\n",
    "# Table: Orders.\n",
    "\n",
    "# +----+------------+ | Id | CustomerId | +----+------------+ | 1 | 3 | | 2 | 1 | +----+------------+ Using the above tables as example, return the following:\n",
    "\n",
    "# +-----------+ | Customers | +-----------+ | Henry | | Max | +-----------+\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank,col,min,max,row_number,max,lag,count,sum\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName(\"Customer\").getOrCreate()\n",
    "spark1 = SparkSession.builder.master(\"local[1]\").appName(\"Order\").getOrCreate()\n",
    "\n",
    "#sample  customer interaction data\n",
    "data = [\n",
    " (1, \"Joe\"),\n",
    " (2, \"Henry\"),\n",
    " (3, \"Sam\"),\n",
    " (4, \"Max\")\n",
    "]\n",
    "schema = StructType([\n",
    " StructField(\"Id\", IntegerType(), nullable=False), StructField(\"NameCust\", StringType(), nullable=False)])\n",
    "\n",
    "\n",
    "data1 = [\n",
    " (1, 3),\n",
    " (2, 1)\n",
    "]\n",
    "schema1 = StructType([\n",
    " StructField(\"Id\", IntegerType(), nullable=False), StructField(\"CustomerId\", IntegerType(), nullable=False)])\n",
    "\n",
    "#create  Dataframe\n",
    "\n",
    "customer_df = spark.createDataFrame(data, schema)\n",
    "customer_df.show(truncate=False)\n",
    "\n",
    "order_df = spark.createDataFrame(data1, schema1)\n",
    "order_df.show(truncate=False)\n",
    "\n",
    "#joining the datasets\n",
    "joined_df = customer_df.join(order_df,customer_df.Id == order_df.CustomerId, 'left')\n",
    "joined_df.show()\n",
    "\n",
    "#filter\n",
    "filterdf = joined_df.filter(joined_df.CustomerId.isNull())\n",
    "filterdf.show()\n",
    "\n",
    "#result\n",
    "result_df = filterdf.select('NameCust')\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea22e8e0-fa89-4d3e-bf89-226649fc9b36",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------------+\n|user_id|activity|activity_date|\n+-------+--------+-------------+\n|1      |login   |2019-05-01   |\n|1      |homepage|2019-05-01   |\n|1      |logout  |2019-05-01   |\n|2      |login   |2019-06-21   |\n|2      |logout  |2019-06-21   |\n|3      |login   |2019-01-01   |\n|3      |jobs    |2019-01-01   |\n|3      |logout  |2019-01-01   |\n|4      |login   |2019-06-21   |\n|4      |groups  |2019-06-21   |\n|4      |logout  |2019-06-21   |\n|5      |login   |2019-03-01   |\n|5      |logout  |2019-03-01   |\n|5      |login   |2019-06-21   |\n|5      |logout  |2019-06-21   |\n+-------+--------+-------------+\n\n+-------+--------+-------------+\n|user_id|activity|activity_date|\n+-------+--------+-------------+\n|      1|   login|   2019-05-01|\n|      1|homepage|   2019-05-01|\n|      1|  logout|   2019-05-01|\n|      2|   login|   2019-06-21|\n|      2|  logout|   2019-06-21|\n|      4|   login|   2019-06-21|\n|      4|  groups|   2019-06-21|\n|      4|  logout|   2019-06-21|\n|      5|   login|   2019-06-21|\n|      5|  logout|   2019-06-21|\n+-------+--------+-------------+\n\n+-------+----------------+\n|user_id|login_date_first|\n+-------+----------------+\n|      1|      2019-05-01|\n|      2|      2019-06-21|\n|      4|      2019-06-21|\n|      5|      2019-06-21|\n+-------+----------------+\n\n+----------------+-----+\n|login_date_first|count|\n+----------------+-----+\n|      2019-05-01|    1|\n|      2019-06-21|    3|\n+----------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Write a pyspark query that reports for every date within at most 90 days from login_date, the number of users that logged in for the first time on that date. Login_date is 2019-06-30.\n",
    "\n",
    "# Traffic table: +---------+----------+---------------+ | user_id | activity | activity_date | +---------+----------+---------------+ | 1 | login | 2019-05-01 | | 1 | homepage | 2019-05-01 | | 1 | logout | 2019-05-01 | | 2 | login | 2019-06-21 | | 2 | logout | 2019-06-21 | | 3 | login | 2019-01-01 | | 3 | jobs | 2019-01-01 | | 3 | logout | 2019-01-01 | | 4 | login | 2019-06-21 | | 4 | groups | 2019-06-21 | | 4 | logout | 2019-06-21 | | 5 | login | 2019-03-01 | | 5 | logout | 2019-03-01 | | 5 | login | 2019-06-21 | | 5 | logout | 2019-06-21 | +---------+----------+---------------+\n",
    "\n",
    "# Result table: +------------+-------------+ | login_date | user_count | +------------+-------------+ | 2019-05-01 | 1 | | 2019-06-21 | 2 | +------------+-------------+\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank,col,min,max,row_number,max,lag,count,sum, date_sub, lit\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName(\"Reports\").getOrCreate()\n",
    "\n",
    "#sample  customer interaction data\n",
    "data = [\n",
    " (1,'login','2019-05-01'),\n",
    " (1,'homepage','2019-05-01'),\n",
    " (1,'logout','2019-05-01'),\n",
    " (2,'login','2019-06-21'),\n",
    " (2,'logout','2019-06-21'),\n",
    " (3,'login','2019-01-01'),\n",
    " (3,'jobs','2019-01-01'),\n",
    " (3,'logout','2019-01-01'),\n",
    " (4,'login','2019-06-21'),\n",
    " (4,'groups','2019-06-21'),\n",
    " (4,'logout','2019-06-21'),\n",
    " (5,'login','2019-03-01'),\n",
    " (5,'logout','2019-03-01'),\n",
    " (5,'login','2019-06-21'),\n",
    " (5,'logout','2019-06-21'),\n",
    "]\n",
    "schema = StructType([\n",
    " StructField(\"user_id\", IntegerType(), nullable=False), StructField(\"activity\", StringType(), nullable=False),\n",
    " StructField(\"activity_date\", StringType(), nullable=False)])\n",
    "\n",
    "\n",
    "\n",
    "#create  Dataframe\n",
    "\n",
    "report_df = spark.createDataFrame(data, schema)\n",
    "report_df.show(truncate=False)\n",
    "\n",
    "\n",
    "# Calculate the date range within at most 90 days from the provided date (2019-06-30)\n",
    "#lit - add literal/constant to existing dataframe\n",
    "#date_sub - difference in dates \n",
    "max_diff  = date_sub(lit('2019-06-30'),90)\n",
    "\n",
    "date_range_df = report_df.filter((col('activity_date') >= max_diff) & (col('activity_date') <= '2019-06-30'))\n",
    "date_range_df.show()\n",
    "\n",
    "\n",
    "# Find the first login date for each user\n",
    "####way to change column name after aggregrate function\n",
    "first_login_df = date_range_df.filter(col('activity') == 'login').groupBy('user_id').agg(min('activity_date')).withColumnRenamed('min(activity_date)', 'login_date_first')\n",
    "first_login_df.show()\n",
    "\n",
    "# Count the number of users for each login date\n",
    "count_df = first_login_df.groupBy('login_date_first').count()\n",
    "\n",
    "# Show the result\n",
    "count_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03fc17c0-3b19-4702-9410-9e93fcef9513",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write a Pyspark query to report the movies with an odd-numbered ID and a description that is not \"boring\".Return the result table in descending order by rating.\n",
    "\n",
    "# Cinema table: +----+------------+-------------+--------+ | id | movie | description | rating | +----+------------+-------------+--------+ | 1 | War | great 3D | 8.9 | | 2 | Science | fiction | 8.5 | | 3 | irish | boring | 6.2 | | 4 | Ice song | Fantacy | 8.6 | | 5 | House card | Interesting | 9.1 | +----+------------+-------------+--------+\n",
    "\n",
    "# Result table: +----+------------+-------------+--------+ | id | movie | description | rating | +----+------------+-------------+--------+ | 5 | House card | Interesting | 9.1 | | 1 | War | great 3D | 8.9 | +----+------------+-------------+--------+\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank,col,min,max,row_number,max,lag,count,sum\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName(\"Cinema\").getOrCreate()\n",
    "\n",
    "#sample  customer interaction data\n",
    "data = [\n",
    " (1,'War','Great 3D',8.9),\n",
    " (2,'Science','Fiction',8.5),\n",
    " (3,'Irish','Boring',6.2),\n",
    " (4,'Ice song','Fantacy',8.6),\n",
    " (5,'House card','Interesting',9.1)\n",
    "]\n",
    "schema = StructType([\n",
    " StructField(\"id\", IntegerType(), nullable=False), StructField(\"movie\", StringType(), nullable=False),\n",
    " StructField(\"description\", StringType(), nullable=False),StructField(\"rating\", FloatType(), nullable=False)])\n",
    "\n",
    "\n",
    "#create  Dataframe\n",
    "\n",
    "cinema_df = spark.createDataFrame(data, schema)\n",
    "cinema_df.show(truncate=False)\n",
    "\n",
    "result_df = cinema_df.filter( (cinema_df.id%2 != 0 ) & (cinema_df.description != 'Boring'))\n",
    "result_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b177a690-c933-4f51-9f5b-7eaafb0370f3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-----------------+\n|user_id|action|timestamp        |\n+-------+------+-----------------+\n|1      | start| 2022-01-01 10:00|\n|1      | end  | 2022-01-01 10:15|\n|2      | start| 2022-01-01 11:00|\n|1      | start| 2022-01-01 11:30|\n|2      | end  | 2022-01-01 11:45|\n|1      | end  | 2022-01-01 12:00|\n+-------+------+-----------------+\n\n+-------+------+-----------------+--------------+\n|user_id|action|        timestamp|unix_timestamp|\n+-------+------+-----------------+--------------+\n|      1| start| 2022-01-01 10:00|          null|\n|      1|   end| 2022-01-01 10:15|          null|\n|      2| start| 2022-01-01 11:00|          null|\n|      1| start| 2022-01-01 11:30|          null|\n|      2|   end| 2022-01-01 11:45|          null|\n|      1|   end| 2022-01-01 12:00|          null|\n+-------+------+-----------------+--------------+\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2484405975002356>:48\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m \u001B[38;5;66;03m#Calculate session duration\u001B[39;00m\n",
       "\u001B[1;32m     47\u001B[0m w  \u001B[38;5;241m=\u001B[39m Window()\u001B[38;5;241m.\u001B[39mpartitionBy(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124muser_id\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39morderBy(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124munix_timestamp\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
       "\u001B[0;32m---> 48\u001B[0m user_df \u001B[38;5;241m=\u001B[39m user_df\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msession_duration\u001B[39m\u001B[38;5;124m'\u001B[39m, when(user_df\u001B[38;5;241m.\u001B[39maction \u001B[38;5;241m==\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mend\u001B[39m\u001B[38;5;124m'\u001B[39m, col(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124munix_timestamp\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mcast(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlong\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;241m-\u001B[39m lag(col(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124munix_timestamp\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mover(w)\u001B[38;5;241m.\u001B[39mcast(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlong\u001B[39m\u001B[38;5;124m'\u001B[39m))))\n",
       "\u001B[1;32m     49\u001B[0m user_df\u001B[38;5;241m.\u001B[39mshow()\n",
       "\u001B[1;32m     51\u001B[0m \u001B[38;5;66;03m#Calculate total session duration per user\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:4758\u001B[0m, in \u001B[0;36mDataFrame.withColumn\u001B[0;34m(self, colName, col)\u001B[0m\n",
       "\u001B[1;32m   4753\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(col, Column):\n",
       "\u001B[1;32m   4754\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n",
       "\u001B[1;32m   4755\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_A_COLUMN\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m   4756\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcol\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(col)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m},\n",
       "\u001B[1;32m   4757\u001B[0m     )\n",
       "\u001B[0;32m-> 4758\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwithColumn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcolName\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcol\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jc\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: Expression 'unix_timestamp#2131L' not supported within a window function.;\n",
       "Window [user_id#2112, action#2113, timestamp#2114, unix_timestamp#2131L, CASE WHEN (action#2113 = end) THEN (cast(unix_timestamp#2131L as bigint) - lag(cast(unix_timestamp#2131L windowspecdefinition(user_id#2112, unix_timestamp#2131L ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) as bigint), -1, null)) END AS session_duration#2153L], [user_id#2112], [unix_timestamp#2131L ASC NULLS FIRST]\n",
       "+- Project [user_id#2112, action#2113, timestamp#2114, unix_timestamp#2131L, action#2113]\n",
       "   +- Project [user_id#2112, action#2113, timestamp#2114, unix_timestamp(timestamp#2114, yyyy-MM-dd HH:mm:ss, Some(Etc/UTC), false) AS unix_timestamp#2131L]\n",
       "      +- LogicalRDD [user_id#2112, action#2113, timestamp#2114], false\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-2484405975002356>:48\u001B[0m\n\u001B[1;32m     46\u001B[0m \u001B[38;5;66;03m#Calculate session duration\u001B[39;00m\n\u001B[1;32m     47\u001B[0m w  \u001B[38;5;241m=\u001B[39m Window()\u001B[38;5;241m.\u001B[39mpartitionBy(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124muser_id\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39morderBy(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124munix_timestamp\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m---> 48\u001B[0m user_df \u001B[38;5;241m=\u001B[39m user_df\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msession_duration\u001B[39m\u001B[38;5;124m'\u001B[39m, when(user_df\u001B[38;5;241m.\u001B[39maction \u001B[38;5;241m==\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mend\u001B[39m\u001B[38;5;124m'\u001B[39m, col(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124munix_timestamp\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mcast(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlong\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;241m-\u001B[39m lag(col(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124munix_timestamp\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mover(w)\u001B[38;5;241m.\u001B[39mcast(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlong\u001B[39m\u001B[38;5;124m'\u001B[39m))))\n\u001B[1;32m     49\u001B[0m user_df\u001B[38;5;241m.\u001B[39mshow()\n\u001B[1;32m     51\u001B[0m \u001B[38;5;66;03m#Calculate total session duration per user\u001B[39;00m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:4758\u001B[0m, in \u001B[0;36mDataFrame.withColumn\u001B[0;34m(self, colName, col)\u001B[0m\n\u001B[1;32m   4753\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(col, Column):\n\u001B[1;32m   4754\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n\u001B[1;32m   4755\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_A_COLUMN\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   4756\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcol\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(col)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m},\n\u001B[1;32m   4757\u001B[0m     )\n\u001B[0;32m-> 4758\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwithColumn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcolName\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcol\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jc\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: Expression 'unix_timestamp#2131L' not supported within a window function.;\nWindow [user_id#2112, action#2113, timestamp#2114, unix_timestamp#2131L, CASE WHEN (action#2113 = end) THEN (cast(unix_timestamp#2131L as bigint) - lag(cast(unix_timestamp#2131L windowspecdefinition(user_id#2112, unix_timestamp#2131L ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) as bigint), -1, null)) END AS session_duration#2153L], [user_id#2112], [unix_timestamp#2131L ASC NULLS FIRST]\n+- Project [user_id#2112, action#2113, timestamp#2114, unix_timestamp#2131L, action#2113]\n   +- Project [user_id#2112, action#2113, timestamp#2114, unix_timestamp(timestamp#2114, yyyy-MM-dd HH:mm:ss, Some(Etc/UTC), false) AS unix_timestamp#2131L]\n      +- LogicalRDD [user_id#2112, action#2113, timestamp#2114], false\n",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: Expression 'unix_timestamp#2131L' not supported within a window function.;\nWindow [user_id#2112, action#2113, timestamp#2114, unix_timestamp#2131L, CASE WHEN (action#2113 = end) THEN (cast(unix_timestamp#2131L as bigint) - lag(cast(unix_timestamp#2131L windowspecdefinition(user_id#2112, unix_timestamp#2131L ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) as bigint), -1, null)) END AS session_duration#2153L], [user_id#2112], [unix_timestamp#2131L ASC NULLS FIRST]\n+- Project [user_id#2112, action#2113, timestamp#2114, unix_timestamp#2131L, action#2113]\n   +- Project [user_id#2112, action#2113, timestamp#2114, unix_timestamp(timestamp#2114, yyyy-MM-dd HH:mm:ss, Some(Etc/UTC), false) AS unix_timestamp#2131L]\n      +- LogicalRDD [user_id#2112, action#2113, timestamp#2114], false\n",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#11\n",
    "# Question: Calculate Average User Session Duration\n",
    "# You have a dataset containing user activity logs in a PySpark DataFrame with the following columns: user_id, timestamp, and action. The action column indicates whether the user started or ended a session. It can have values 'start' or 'end'. Your task is to calculate the average duration of user sessions.\n",
    "\n",
    "# user_id | timestamp          | action\n",
    "# ------- | ------------------ | ------\n",
    "# 1       | 2022-01-01 10:00   | start\n",
    "# 1       | 2022-01-01 10:15   | end\n",
    "# 2       | 2022-01-01 11:00   | start\n",
    "# 1       | 2022-01-01 11:30   | start\n",
    "# 2       | 2022-01-01 11:45   | end\n",
    "# 1       | 2022-01-01 12:00   | end\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank,col,min,max,row_number,lag,count,sum, to_timestamp, when,cast, to_utc_timestamp\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName(\"User_session\").getOrCreate()\n",
    "\n",
    "#sample  customer interaction data\n",
    "data = [\n",
    "        (1,\" start\",\" 2022-01-01 10:00\"),\n",
    "        (1,\" end\",\" 2022-01-01 10:15\"),\n",
    "        (2,\" start\",\" 2022-01-01 11:00\"),\n",
    "        (1,\" start\",\" 2022-01-01 11:30\"),\n",
    "        (2,\" end\", \" 2022-01-01 11:45\"),\n",
    "        (1,\" end\",\" 2022-01-01 12:00\")\n",
    "]\n",
    "schema = StructType([\n",
    " StructField(\"user_id\", IntegerType(), nullable=False), StructField(\"action\", StringType(), nullable=False),\n",
    " StructField(\"timestamp\", StringType(), nullable=False)])\n",
    "\n",
    "#create  Dataframe\n",
    "\n",
    "user_df = spark.createDataFrame(data, schema)\n",
    "user_df.show(truncate=False)\n",
    "\n",
    "#Logic\n",
    "\n",
    "user_df = user_df.withColumn('unix_timestamp',unix_timestamp(col('timestamp'),'yyyy-MM-dd HH:mm:ss'))\n",
    "user_df.show()\n",
    "\n",
    "\n",
    "#Calculate session duration\n",
    "w  = Window().partitionBy('user_id').orderBy('unix_timestamp')\n",
    "user_df = user_df.withColumn('session_duration', when(user_df.action =='end', col('unix_timestamp').cast('long') - lag(col('unix_timestamp').over(w).cast('long'))))\n",
    "user_df.show()\n",
    "\n",
    "#Calculate total session duration per user\n",
    "user_session_duration = user_df.groupBy('user_id').agg(sum('session_duration').alias('total_duration_per_user'))\n",
    "user_session_duration.show()\n",
    "\n",
    "#Calculate Averagre Session duration\n",
    "Avg_duration = user_session_duration.selectExpr('user_id','total_duration_per_user / count(user_id) as Avg_duration')\n",
    "Avg_duration.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24c94e4b-546a-4c59-b381-c197fa656ef5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 11\n",
    "# Sample Question: Analyzing Sales Data\n",
    "# Problem Statement: You are given a CSV file containing sales data with the following columns: date, product_id, quantity_sold, and price_per_unit. Your task is to write a PySpark program to analyze this sales data and produce the following insights:\n",
    "# •\tTotal revenue for each product.\n",
    "# •\tTotal revenue for each date.\n",
    "# •\tTop 5 selling products based on the quantity sold.\n",
    "\n",
    "# date,product_id,quantity_sold,price_per_unit\n",
    "# 2023-08-01,101,5,10.0\n",
    "# 2023-08-01,102,3,15.0\n",
    "# 2023-08-02,101,8,10.0\n",
    "# 2023-08-02,103,6,12.0\n",
    "# 2023-08-03,102,4,15.0\n",
    "# 2023-08-03,104,2,20.0\n",
    "\n",
    "# Expected output\n",
    "\n",
    "# Total revenue per product:\n",
    "# +----------+-------------+\n",
    "# |product_id|total_revenue|\n",
    "# +----------+-------------+\n",
    "# |       101|         90.0|\n",
    "# |       102|         75.0|\n",
    "# |       103|         72.0|\n",
    "# |       104|         40.0|\n",
    "# +----------+-------------+\n",
    "\n",
    "# Total revenue per date:\n",
    "# +----------+-------------+\n",
    "# |      date|total_revenue|\n",
    "# +----------+-------------+\n",
    "# |2023-08-01|         135.0|\n",
    "# |2023-08-02|         150.0|\n",
    "# |2023-08-03|         110.0|\n",
    "# +----------+-------------+\n",
    "\n",
    "# Top 5 selling products:\n",
    "# +----------+-------------+\n",
    "# |product_id|total_quantity_sold|\n",
    "# +----------+-------------+\n",
    "# |       101|                13|\n",
    "# |       102|                 7|\n",
    "# |       103|                 6|\n",
    "# |       104|                 2|\n",
    "# +----------+-------------+\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ceb4eca-9687-4738-aca2-7beba971a823",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------------+--------------+\n|date      |product_id|quantity_sold|price_per_unit|\n+----------+----------+-------------+--------------+\n|2023-08-01|101       |5            |10.0          |\n|2023-08-01|102       |3            |15.0          |\n|2023-08-02|101       |8            |10.0          |\n|2023-08-02|103       |6            |12.0          |\n|2023-08-03|102       |4            |15.0          |\n|2023-08-03|104       |2            |20.0          |\n+----------+----------+-------------+--------------+\n\n+----------+-------------+\n|product_id|total_revenue|\n+----------+-------------+\n|       101|        130.0|\n|       102|        105.0|\n|       103|         72.0|\n|       104|         40.0|\n+----------+-------------+\n\n+----------+-------------+\n|      date|total_revenue|\n+----------+-------------+\n|2023-08-01|         95.0|\n|2023-08-02|        152.0|\n|2023-08-03|        100.0|\n+----------+-------------+\n\n+----------+-------------------+\n|product_id|total_quantity_sold|\n+----------+-------------------+\n|       101|               13.0|\n|       102|                7.0|\n|       103|                6.0|\n|       104|                2.0|\n+----------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank,col,min,max,row_number,max,lag,count,sum\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName(\"User_session\").getOrCreate()\n",
    "\n",
    "#sample  customer interaction data\n",
    "data = [\n",
    "        ('2023-08-01',101,5,10.0),\n",
    "        ('2023-08-01',102,3,15.0),\n",
    "        ('2023-08-02',101,8,10.0),\n",
    "        ('2023-08-02',103,6,12.0),\n",
    "        ('2023-08-03',102,4,15.0),\n",
    "        ('2023-08-03',104,2,20.0)\n",
    "]\n",
    "schema = StructType([\n",
    " StructField(\"date\", StringType(), nullable=False), StructField(\"product_id\", StringType(), nullable=False),\n",
    " StructField(\"quantity_sold\", StringType(), nullable=False), StructField(\"price_per_unit\", StringType(), nullable=False)])\n",
    "\n",
    "#create  Dataframe\n",
    "\n",
    "user_df = spark.createDataFrame(data, schema)\n",
    "user_df.show(truncate=False)\n",
    "\n",
    "#Logic\n",
    "# •\tTotal revenue for each product.\n",
    "user_df.withColumn('total_revenue',col('quantity_sold') * col('price_per_unit')).groupBy('product_id').agg(sum('total_revenue').alias('total_revenue')).show()\n",
    "\n",
    "# Total revenue per date:\n",
    "user_df.withColumn('total_revenue_per_date',col('quantity_sold') * col('price_per_unit')).groupBy('date').agg(sum('total_revenue_per_date').alias('total_revenue')).show()\n",
    "\n",
    "# Top 5 selling products:\n",
    "user_df.groupBy('product_id').agg(sum('quantity_sold').alias('total_quantity_sold')).limit(5).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b179a91-3677-42e0-8de5-10d128667ea4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n|user_id|          timestamp|\n+-------+-------------------+\n|  user1|2023-08-26 10:10:00|\n|  user1|2023-08-26 10:10:25|\n|  user2|2023-08-26 12:00:00|\n|  user2|2023-08-26 12:10:00|\n|  user1|2023-08-27 14:30:00|\n|  user1|2023-08-28 16:00:00|\n|  user2|2023-08-27 16:30:00|\n|  user1|2023-08-29 18:00:00|\n+-------+-------------------+\n\n+-------+-------------------+----------+\n|user_id|          timestamp|      date|\n+-------+-------------------+----------+\n|  user1|2023-08-26 10:10:00|2023-08-26|\n|  user1|2023-08-26 10:10:25|2023-08-26|\n|  user2|2023-08-26 12:00:00|2023-08-26|\n|  user2|2023-08-26 12:10:00|2023-08-26|\n|  user1|2023-08-27 14:30:00|2023-08-27|\n|  user1|2023-08-28 16:00:00|2023-08-28|\n|  user2|2023-08-27 16:30:00|2023-08-27|\n|  user1|2023-08-29 18:00:00|2023-08-29|\n+-------+-------------------+----------+\n\n+-------+-------------------+----------+---------+\n|user_id|          timestamp|      date|date_diff|\n+-------+-------------------+----------+---------+\n|  user1|2023-08-26 10:10:00|2023-08-26|     null|\n|  user1|2023-08-26 10:10:25|2023-08-26|        0|\n|  user1|2023-08-27 14:30:00|2023-08-27|        1|\n|  user1|2023-08-28 16:00:00|2023-08-28|        1|\n|  user1|2023-08-29 18:00:00|2023-08-29|        1|\n|  user2|2023-08-26 12:00:00|2023-08-26|     null|\n|  user2|2023-08-26 12:10:00|2023-08-26|        0|\n|  user2|2023-08-27 16:30:00|2023-08-27|        1|\n+-------+-------------------+----------+---------+\n\n+-------+--------------------+\n|user_id|max_consecutive_days|\n+-------+--------------------+\n|  user1|                   3|\n|  user2|                   1|\n+-------+--------------------+\n\n+-------+--------------------+\n|user_id|max_consecutive_days|\n+-------+--------------------+\n|  user1|                   3|\n|  user2|                   1|\n+-------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "#12\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"TopConsecutiveUsers\").getOrCreate()\n",
    "\n",
    "# Define the schema for your data\n",
    "schema = [\"user_id\", \"timestamp\"]\n",
    "data = [\n",
    "    (\"user1\", \"2023-08-26 10:10:00\"),\n",
    "    (\"user1\", \"2023-08-26 10:10:25\"),\n",
    "    (\"user2\", \"2023-08-26 12:00:00\"),\n",
    "    (\"user2\", \"2023-08-26 12:10:00\"),\n",
    "    (\"user1\", \"2023-08-27 14:30:00\"),\n",
    "    (\"user1\", \"2023-08-28 16:00:00\"),\n",
    "    (\"user2\", \"2023-08-27 16:30:00\"),\n",
    "    (\"user1\", \"2023-08-29 18:00:00\"),\n",
    "]\n",
    "\n",
    "# Create a DataFrame from the data\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "df.show()\n",
    "\n",
    "# Convert the timestamp column to a date type\n",
    "df = df.withColumn('date',F.to_date('timestamp'))\n",
    "df.show()\n",
    "# Define a window specification to partition by user and order by date\n",
    "w = Window().partitionBy('user_id').orderBy('date')\n",
    "\n",
    "# Calculate the difference between consecutive dates\n",
    "df = df.withColumn('date_diff', F.datediff('date',F.lag('date').over(w)))\n",
    "df.show()\n",
    "# Identify consecutive login sequences\n",
    "df = df.withColumn('is_consecutive',F.when(F.col('date_diff') == 1, 1).otherwise(0))\n",
    "\n",
    "# Calculate the cumulative sum of consecutive logins\n",
    "df= df.withColumn('cumulative_sum',F.sum('is_consecutive').over(w))\n",
    "\n",
    "# Find the maximum consecutive logins for each user\n",
    "max_consecutive_logins = df.groupBy('user_id').agg(max('cumulative_sum').alias('max_consecutive_days'))\n",
    "max_consecutive_logins.show()\n",
    "\n",
    "# Find the top 5 users with the most consecutive logins\n",
    "top_5 = max_consecutive_logins.orderBy(F.desc('max_consecutive_days')).limit(5)\n",
    "\n",
    "# Show the result\n",
    "top_5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4ad6a02-749c-4572-bbc8-e679325036c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 13\n",
    "# rolling averages\n",
    "\n",
    "#  Qu: real-time scenario where you have a dataset of stock price data, and you want to calculate the rolling average stock price for each stock within a specific time window.\n",
    "\n",
    "#  A rolling average, also known as a moving average, is a statistical calculation used in time series analysis to smooth out fluctuations in data over a specified period. It is a commonly used technique in various fields, including finance, economics, signal processing, and data analysis. The rolling average provides a more stable and less noisy representation of data, making it easier to identify trends and patterns.\n",
    "\n",
    "# Here's how a rolling average is calculated:\n",
    "\n",
    "# 1. Select a Window Size: You choose a fixed window size (e.g., 3 days, 7 days, 30 days) that defines the number of data points to include in each average calculation.\n",
    "\n",
    "# 2. Slide the Window: Starting from the first data point, you slide the window over the time series data one step at a time. At each step, you include the data points within the window and calculate their average.\n",
    "\n",
    "# 3. Smoothed Data: The result of this calculation is a new time series, often referred to as the rolling average or moving average. Each point in the rolling average represents the average value of the data within the window at that point in time.\n",
    "\n",
    "# The rolling average is useful for various purposes:\n",
    "\n",
    "# 1. Noise Reduction: It helps reduce random fluctuations or noise in time series data, making underlying trends more apparent.\n",
    "\n",
    "# 2. Trend Identification: By smoothing the data, rolling averages make it easier to identify long-term trends, upward or downward movements, or cyclic patterns.\n",
    "\n",
    "# 3. Seasonal Adjustment: In economics and finance, rolling averages are often used to seasonally adjust data by removing the seasonal components.\n",
    "\n",
    "# 4. Forecasting: Rolling averages can be used as a basis for forecasting future values in time series analysis.\n",
    "\n",
    "# Example:\n",
    "# Let's say you have daily stock price data for a company, and you calculate a 7-day rolling average. The rolling average for a particular day would be the average of the stock prices for that day and the previous six days. This smooths out daily fluctuations, making it easier to see the overall trend in the stock's price.\n",
    "\n",
    "# In PySpark, as shown in a previous response, you can use window functions to calculate rolling averages within a specified window size for time-based data analysis."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Pyspark 2",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
